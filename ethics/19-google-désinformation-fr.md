# La désinformation scientifique de Google que j’ai débunkée

En 2010, les historiens des sciences Naomi Oreskes et Erik M. Conway publient “Marchand de doutes”, un ouvrage qui expose l’étendue de la désinformation scientifique qui vise à nier l’ampleur du changement climatique et ses conséquences probables sur la sécurité des sociétés humaines. Ainsi, certains instituts et experts du domaine ont visiblement délibérément biaisé leurs recherches, pour “nourrir la controverse” et banaliser le statu quo. Dans leur livre très documenté, Oreskes et Conway trace en particulier des parallèles saisissants avec d’autres cas avérés précédemment, comme la désinformation scientifique sur le tabagisme, les pluies acides et le trou dans la couche d'ozone. Dans tous ces cas, la communauté scientifique a été empoisonnée par des membres malveillants, qui maintenaient le doute sur la dangerosité de produits industriels et normalisaient l’inaction malgré cette dangerosité.

https://fr.wikipedia.org/wiki/Les_Marchands_de_doute 

Si une énorme partie des sciences est épargnée de telles trahisons, notamment les sciences fondamentales qui n’ont que peu d’applications, la présence d’énormes enjeux industriels ou géopolitiques dans certains secteurs doit absolument appeler à la vigilance. 
https://tournesol.app/entities/yt:rStL7niR7gs
Il ne faut ainsi pas perdre de vue que la science est menée par des humains, qui peuvent être mis sous pression, notamment sous pression de publication, 
https://tournesol.app/entities/yt:fCEeNo3j6dM
voire être corrompus, ou plus simplement biaisés par l’influence parfois malsaine de “stars” de leurs domaines.
https://tournesol.app/entities/yt:-p92CwIHgIA 

Or malheureusement, dans mon domaine de recherche, à savoir l’intelligence artificielle, et en particulier la sécurité en machine learning, les enjeux industriels et géopolitiques sont absolument énormes — et le phénomène de starification très répandu. 
https://tournesol.app/entities/yt:8YHFGYh1c6I 
Après tout, on parle là d’entreprises plus puissantes encore que les industries pétrolières, rien qu'en termes de chiffres d'affaires ! À cela s’ajoute la pression pour publier, avec les salaires mirobolants à la clé pour ceux qui y parviennent. 
https://en.wikipedia.org/wiki/List_of_public_corporations_by_market_capitalization
https://www.nytimes.com/2018/04/19/technology/artificial-intelligence-salaries-openai.html

Voilà qui me semble avoir dangereusement poussé quelques uns de mes pairs, notamment quelques uns qui bossent dans ou avec des entreprises privées, à produire, diffuser et accepter des désinformations scientifiques extrêmement dangereuses — des désinformations qui justifient le déploiement précipité et massif d’algorithmes beaucoup trop peu sécurisés, et qui sont hackés par des campagnes de désinformation depuis plus d’une décennie, conduisant ainsi à la montée de l’autoritarisme partout dans le monde, et même à des génocides, au Myanmar, en Éthiopie et en Ukraine. 
https://tournesol.app/entities/yt:lYXQvHhfKu
https://tournesol.app/entities/yt:utWMGi8HTjY 
https://tournesol.app/entities/yt:ocXOPEnL5ZE 
https://www.v-dem.net/democracy_reports.html

De façon plus dérangeante encore, l’écrasante majorité de la communauté scientifique me semble avoir fini par normaliser, voire à encourager, l’inattention à cette désinformation scientifique très préoccupante, et à ses conséquences sociales désastreuses sur des millions de vies humaines.
https://tournesol.app/entities/yt:zPTTxsdtI3A M 

Aujourd’hui, je vais vous parler de quatre articles scientifiques publiés dans des conférences prestigieuses avec comité de relecture, qui me semblent être très clairement de la désinformation scientifique dangereuse. Et pour être clair, je ne prétends pas que le contenu des articles est faux, mais plutôt que la présentation de ces articles, en particulier les titres, résumés et introductions, tout ça est volontairement et dangereusement trompeur, à l’instar d’une publication qui signalerait le refroidissement d’une région de la planète, et dont le titre serait “tout le globe ne se réchauffe pas”. 
https://dl.acm.org/doi/abs/10.1145/3494834.3500240 
https://ieeexplore.ieee.org/document/9833647/ 
https://dl.acm.org/doi/10.1145/3465456.3467580 
https://openreview.net/pdf?id=bVuP3ltATMz 

Étrangement, tous les articles les plus problématiques que j’ai trouvés sont tous publiés avec au moins un employé de Google parmi les auteurs — une entreprise qui s’est précédemment opposée à la publication d’articles scientifiques par ses chercheurs, et qui a explicitement exigé d’eux qu’ils “adoptent un ton positif” dans leurs publications scientifiques. 
https://tournesol.app/entities/yt:HbFadtOxs4k

“Strike a positive tone”
https://www.nbcnews.com/tech/tech-news/google-told-its-scientists-strike-positive-tone-ai-research-documents-n1252240 

Mais surtout, tous ces articles prétendent à tort résoudre les problèmes de sécurité du machine learning ; ou au moins, ils visent à nier ces problèmes — à l’instar de lobbies du pétrole qui cherchent à nier la gravité du changement climatique ou à affirmer que des solutions techniques finiront par être trouvées. Mais surtout, on va voir que cette désinformation est jusque là terriblement efficace, en dévalorisant certaines lignes de recherche sur la sécurité, ou en produisant des mésinformations devenues malheureusement très répandues dans la communauté scientifique. Or, bien entendu, la désinformation dont on va parler est restreinte à ce que j’ai trouvé, lu et pris le temps de débunker, avec mon temps très limité et très occupé par beaucoup d’autres priorités…
https://dl.acm.org/doi/abs/10.1145/3494834.3500240 
https://ieeexplore.ieee.org/document/9833647/ 
https://dl.acm.org/doi/10.1145/3465456.3467580 
https://openreview.net/pdf?id=bVuP3ltATMz 

Dans cette vidéo, je vous parlerai aussi de l’énorme travail de débunking scientifique que mes fantastiques collègues et moi avons effectué, en publiant des travaux qui contredisent cette désinformation dans les revues scientifiques les plus prestigieuses en machine learning, notamment NeurIPS et ICML — même si nos articles reçoivent beaucoup moins de citations et d’attention de la part des académiques, que la désinformation produite par Google et célébrée à tort par la communauté scientifique… 
https://proceedings.neurips.cc/paper/2021/hash/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Abstract.html
https://proceedings.mlr.press/v162/farhadkhani22b.html 
https://arxiv.org/abs/2209.15259 

Si vous êtes chercheur en informatique, je vous invite à exiger beaucoup plus de rigueur de vous-même et de vos collègues, surtout vis-à-vis des affirmations selon lesquelles des problèmes de sécurité seraient résolus ou peu crédibles. La publication scientifique de ces affirmations aura des conséquences majeures sur les algorithmes déployés à très grande échelle par Google et compagnie, et risquent alors d’amplifier des tensions géopolitiques déjà très préoccupantes. Je vous invite à être particulièrement rigoureux au moment de faire la review des articles de vos pairs, de sélectionner des conférenciers à inviter et de choisir qui recruter ou promouvoir. En plus de potentiellement gober et propager de la désinformation scientifique, un manque de vigilance peut aussi et surtout conduire à davantage de proliférations d’algorithmes opaques, mal sécurisés et potentiellement très dangereux, sous prétexte que leurs principes ont été validés par la communauté scientifique.
https://tournesol.app/entities/yt:lYXQvHhfKuM 
https://tournesol.app/entities/yt:U_7CGl6VWaQ 

Et si vous n’êtes pas chercheur en informatique, vous pouvez toujours aider à alerter la communauté scientifique sur sa vulnérabilité à la désinformation scientifique de Google, en partageant massivement cette vidéo, notamment avec des chercheurs en informatique, mais aussi en interpelant des journalistes et des politiciens, ou encore en participant à des projets de recherche participative et éthique en machine learning, comme le projet Tournesol. Je reviendrai plus longuement sur les actions possibles en fin de vidéo.
https://tournesol.app/ 

Alors, la vidéo est un peu longue, donc juste pour vous y retrouver, on va d’abord parler des désinformations autour de la protection des informations sensibles par des algorithmes d’apprentissage, avant de parler de la sécurité de ces algorithmes. Et puis, enfin il y aura une conclusion. Chaque section est ensuite sous-divisée en sous-section, dont vous trouverez les time codes en description. Allez, on est partis…

## La protection des informations sensibles
### Le machine learning apprend les données personnelles

La désinformation scientifique la plus simple à comprendre est celle de la protection des informations sensibles. Jusque là, les régulations comme le RGPD, le règlement général sur la protection des données, se sont surtout concentrées sur la protection des données des utilisateurs, avec l’hypothèse sous-jacente selon laquelle cela suffirait à protéger nos sociétés contre la révélation publique d’informations sensibles. 
https://tournesol.app/entities/yt:OUMGp3HHel4 

Sauf que le lien entre protection des données personnelles, par exemple sur votre téléphone ou dans votre compte Google, et protection des informations sensibles indirectement extraites de ces données personnelles est plus complexe qu’on pourrait le croire, surtout dès lors que des algorithmes utilisent les données personnelles pour répondre à des requêtes d’autres utilisateurs du web. Et notez que ceci peut survenir même avec des algorithmes ultra simples : lorsque WhatsApp vous dit que votre interlocuteur a bien reçu votre message, il révèle une information que cet interlocuteur aurait potentiellement préféré ne pas révéler. L’information sensible, c’est-à-dire l’information qu’une personne ne souhaiterait pas voir diffusée, est alors enregistrée indirectement dans les données d’une autre personne.

Ceci dit, le problème de l’exploitation algorithmique des données personnelles est bien entendu bien plus important dans le cas du machine learning. Après tout, ces algorithmes d’apprentissage sont littéralement conçus pour apprendre des données. Si ces données contiennent des informations sensibles, alors l’algorithme aura littéralement appris ces informations sensibles. Or, de façon très problématique et dangereuse, l’algorithme qui contient potentiellement les informations sensibles n’est généralement pas considéré être une donnée personnelle, et RGPD est alors très flou vis-à-vis des droits et des devoirs de cet algorithme.

Pourtant, si l’algorithme a appris des informations sensibles, les actions ensuite entreprises par l’algorithme peuvent largement trahir ces informations sensibles. Par exemple, il suffit de regarder les recommandations faites par l’algorithme de YouTube à un utilisateur pour connaître les habitudes de consommation de l’utilisateur. Bref, tout objet obtenu à partir d’une exploitation de données personnelles, y compris un objet aussi simple qu’une recommandation, est un vecteur de fuite potentielle d’informations sensibles.

Pire encore, les algorithmes de langage sont souvent conçus pour littéralement recracher ce qu’ils ont appris, avec presque toujours très peu, sinon aucune, distinction entre les informations publiques et les informations sensibles. En demandant à ces algorithmes de remplir des formulaires semi-remplis, des chercheurs ont montré qu’il était parfois possible de récupérer les informations personnelles de vrais individus humains, tandis que des utilisateurs d’un algorithme de langage informatique ont montré que l’algorithme exposait des clés secrètes.
https://dl.acm.org/doi/10.1145/3442188.3445922 
https://arxiv.org/abs/2012.07805 
https://news.ycombinator.com/item?id=27736460 
https://www.reddit.com/r/ProgrammerHumor/comments/u4dh2o/github_copilot_just_leaked_someones_api_key/ 

De façon plus préoccupante encore, au fur et à mesure que les algorithmes de langage se développent, il faut s’attendre à ce que nos téléphones disposent petit à petit d’algorithmes de langage très sophistiqués pour faire l’autocomplétion de nos messages sur nos claviers intelligents. Dès lors, il pourrait un jour être possible de taper “Monsieur Phi a dit hier soir à sa femme que”, et de lire l’autocomplétion par Google GBoard pour extraire des informations potentiellement très sensibles. Le déploiement massif de tels algorithmes serait alors très dangereux. 

Devant ces risques évidents, Google a bien dû s’attaquer à la sécurisation de ses algorithmes. Et donc, comment s’y prennent-ils ? L’ont-ils fait avec sérieux, rigueur et transparence ? Ont-ils été à la hauteur des énormes enjeux sociaux en jeu ?

### L’apprentissage fédéré
En 2015, Google a proposé un algorithme d’apprentissage distribué, qu’ils ont appelé apprentissage fédéré ou federated learning en anglais, capable d’apprendre des données des utilisateurs sans jamais que ces données ne soient transférées aux serveurs de Google. En gros, au lieu d’envoyer les données, chaque téléphone des utilisateurs va utiliser des algorithmes liés aux fameux réseaux de neurones, pour calculer ce qu’on appelle un modèle ou un gradient ; et c’est cet objet qui est transféré aux serveurs de Google.
https://arxiv.org/abs/1511.03575 

Les chercheurs de Google ont ensuite déployé leur système d’apprentissage fédéré sur les téléphones de millions d’utilisateurs pour apprendre ce que ces utilisateurs tapent sur leurs claviers intelligents GBoard. Des données avec des informations potentiellement incroyables sensibles. Réfléchissez-y. Il n’y a peut-être rien de plus intime que ce que vous tapez sur le clavier de votre téléphone, surtout sur des messageries privées comme Signal ou WhatsApp… Qui plus est, l’information dans ces données peut être aussi sensible pour votre interlocuteur que pour vous ; elle peut même être sensible pour un tiers !
https://www.nbcnews.com/tech/tech-news/facebook-turned-chat-messages-mother-daughter-now-charged-abortion-rcna42185 

Et bien sûr, tout cela est fait sans le consentement éclairé des utilisateurs — en tout cas les articles ne parlent pas du tout de consentement. Je cite : “pour cette étude, des entrées ont été collectées de la population anglophone d’utilisateurs de GBoard aux États-Unis”.  
https://arxiv.org/abs/1812.02903 


En fait, si vous utilisez un clavier GBoard, par défaut, tout ce que vous tapez est très certainement appris par un algorithme de Google. Pensez-y, notamment si vous utilisez des messageries sécurisées comme Signal. Avant que vos messages ne soient chiffrés, vous les tapez en clair sur un clavier qui apprend très probablement de ce que vous tapez… et vu le manque de sécurité de GBoard, une “gaffe” de cette IA pourrait un jour faire fuiter vos communications…
https://arxiv.org/abs/1811.03604 

Bref. Je vous invite à envisager l’utilisation d’alternatives Open Source comme OpenBoard ou AnySoftKeyboard. Tout en restant vigilant. Rien ne garantit que le code exécuté sur le téléphone soit celui publié sur github. En fait, pour les trucs ultra-sensibles, vous pouvez carrément désactiver le côté “intelligent” du clavier, et surtout utiliser une machine moins vulnérable que le téléphone, comme un ordinateur.
https://play.google.com/store/apps/details?id=org.dslul.openboard.inputmethod.latin&gl=US 
https://play.google.com/store/apps/details?id=com.menny.android.anysoftkeyboard&gl=US 

### La désinformation sur la privacy de l’apprentissage fédéré
Mais ce n’est pas là le plus grave. Ce qui est extrêmement dangereux, c’est que Google est parvenu à propager la mésinformation scientifique selon laquelle l’apprentissage fédéré ainsi conçu, et si utile pour le business lucratif des publicités ciblées, protège les données des utilisateurs. Je cite un article de blog de Google : “l’apprentissage fédéré permet des modèles plus intelligents, avec moins de latence et moins de consommation énergétique, tout en garantissant la privacy.” 

“Federated Learning allows for smarter models, lower latency, and less power consumption, all while ensuring privacy”
https://ai.googleblog.com/2017/04/federated-learning-collaborative.html

“Tout en garantissant la privacy”… Quelle désinformation !

Et cette désinformation s’est retrouvée publiée dans la littérature scientifique, dans cet article publié à CCS 2017 au titre trompeur “Aggrégation sécurisée pratique pour du machine learning qui préserve la privacy", et avec l’affirmation trompeuse dans le résumé “nous prouvons la sécurité de notre protocole dans des contextes d’utilisateurs honnêtes-mais-curieux et activement adversiariaux”.
https://dl.acm.org/doi/abs/10.1145/3133956.3133982 

“Practical Secure Aggregation for Privacy-Preserving Machine Learning”
“We prove the security of our protocol in the honest-but-curious and active adversary settings,”

Alors, je n’ai pas traduit le mot “privacy”, parce qu’il y a des subtilités légales dans le choix de traduction ; mais pour le chercheur en machine learning qui connaît mal le sujet, ou pour le législateur qui lit en diagonale, cette phrase donne l’impression que l’apprentissage fédéré garantit la protection des informations sensibles. Il s’agit là, je pense, de l’une des plus graves désinformations scientifiques d’aujourd’hui. 

Et pourtant, cette désinformation est triviale à débunker : en effet, l’apprentissage fédéré est tout simplement une procédure plus sophistiquée pour conduire au même résultat que l’apprentissage centralisé classique. Dès lors, un algorithme fédéré aura appris la même chose que ce qu’aura appris un algorithme centralisé, y compris si des mesures de chiffrements à base de cryptographie homomorphe ou de secrets partagés sont utilisé au milieu, comme c’est le cas de l’article de Google. 
https://arxiv.org/abs/2111.07380 
https://dl.acm.org/doi/10.5555/3295222.3295285 

Or, depuis environ 5 ans, de nombreuses recherches théoriques et empiriques n’ont cessé de montrer que les algorithmes d’apprentissage modernes obtiennent leurs meilleures performances lorsqu’ils mémorisent leurs données d’apprentissage — un phénomène parfois appelé “double descent”. Dès lors, l’apprentissage fédéré à son meilleur — et qui aujourd’hui en machine learning ne cherche pas à optimiser son apprentissage ? — cet apprentissage fédéré produira in fine un algorithme qui aura mémorisé toutes les données des utilisateurs.
https://dl.acm.org/doi/10.1145/3446776 
https://www.pnas.org/doi/10.1073/pnas.1903070116 
http://proceedings.mlr.press/v89/belkin19a.html 
https://openreview.net/forum?id=B1g5sA4twr 

Dès lors, il pourrait être possible pour n’importe qui de décortiquer cet algorithme pour récupérer des informations sensibles. Pire, s’il s’agit d’un algorithme de traitement de langage, il pourrait suffire de poser des questions à l’algorithme pour que celui-ci réponde en révélant les informations sensibles.

Et les auteurs de l’article de désinformation s’en sont bien rendus compte, mais ne le signalent que dans le coeur de l’article, où ils écrivent : “quand le protocole est exécuté avec un seuil t, la vue conjointe du server et de n’importe quel sous-ensemble de t utilisateurs (honnêtes) ne fait fuiter aucune information à propos des utilisateurs, à part ce qui peut être inféré du résultat du calcul”. 
https://dl.acm.org/doi/pdf/10.1145/3133956.3133982 

“when executing the protocol with threshold t, the joint view of the server and any set of less than t (honest) users does not leak any information about the other users’ inputs, besides what can be inferred from the output of the computation”

L’aveu de faiblesse, dissimulé dans une précision après une virgule au milieu de l’article, est en fait énorme. En particulier, le dispositif n’offre finalement aucune garantie globale de protection des informations sensibles. Mais ça, les académiques, les journalistes et les législateurs paresseux qui se contentent du titre, du résumé et de l’introduction, comme c’est mon cas aussi trop souvent, on ne s’en rendra pas compte !

Or les conséquences d’une propagation massive d’informations sensibles par les algorithmes d’apprentissage pourraient être catastrophiques. Imaginez le bouleversement social que cela représenterait si tout le monde pouvait tout à coup espionner n’importe qui d’autres. La révélation soudaine de nombreuses trahisons pourrait exacerber les tensions, et sérieusement augmenter les risques de violences armées. 
https://github.com/lenhoanglnh/SmartPoop/blob/main/French/4-Fuites.md 

Par ailleurs, les secrets d’affaire et d’État pourraient alors être exploités par des entités malveillantes. Pire encore, des mots de passe de systèmes critiques, notamment pour prendre le contrôle de données médicales, de centrales nucléaires ou de systèmes bancaires, ou des informations sensibles des humains en charge de la sécurité de ces systèmes d’information, pourraient être exploités par des acteurs malveillants. On parle là d’un enjeu de sécurité nationale, voire de paix mondiale.
https://nypost.com/2022/08/13/how-twitter-employees-ratted-out-users-to-saudi-arabia/ 
https://www.vice.com/en/article/qjk3wb/facebook-engineers-admit-they-dont-know-what-they-do-with-your-data 

Bref. L’apprentissage fédéré, même s’il est combiné avec des affaires de chiffrement homomorphe ou de secrets partagés n’offre aucune garantie de protection des informations sensibles. Au mieux, il faut y voir un hack technique pour faire du machine learning en se conformant à une version faible, et très maladroite, du RGPD, le règlement général sur la protection des données. Au pire, il faut y voir une désinformation que Google a voulu faire avaler à la communauté scientifique, avec succès, pour légitimer le déploiement massif de technologies de surveillance qui permettront une publicité toujours mieux ciblée.
https://dl.acm.org/doi/abs/10.1145/3532105.3536394

### La communauté scientifique amplifie cette désinformation
Et pourtant, de façon très décevante, cette désinformation selon laquelle l’apprentissage fédéré protège les informations sensibles a été reprise, amplifiée et normalisée par beaucoup trop de publications scientifiques. L’article scientifique très dangereux de Google de 2017 a été cité près de 1600 fois par les publications d’autres chercheurs académiques d’après Google Scholar – ce qui dans le monde de la recherche, est vraiment énorme !
​​https://scholar.google.ch/scholar?hl=en&as_sdt=0%2C5&q=Practical+secure+aggregation+for+privacy-preserving+machine+learning&btnG= 

Pire encore, en recherchant entre guillemets “Federated learning is a privacy” sur Google Scholar, on se rend compte que la désinformation de Google est reprise telle quelle et sans justification, dans un nombre ahurissant d’articles scientifiques, publiées dans des revues scientifiques prestigieuses comme IEEE Security & Privacy, IEEE Journal of Selected Areas in Communications, High-Confidence Computing et Nature Communications, EdgeSys, SPML, entre autres. Vraiment, le peer-review en machine learning n’est pas une garantie de qualité… 
https://scholar.google.ch/scholar?hl=en&as_sdt=0%2C5&q=%22federated+learning+is+a+privacy%22&btnG= 
https://ieeexplore.ieee.org/abstract/document/9308910/
https://ieeexplore.ieee.org/abstract/document/9276464
https://www.sciencedirect.com/science/article/pii/S266729522100009X 
https://www.nature.com/articles/s41467-022-29763-x 
https://dl.acm.org/doi/abs/10.1145/3434770.3459734 
https://dl.acm.org/doi/abs/10.1145/3432291.3432303 

Et de façon terrifiante, cette désinformation scientifique a été utilisée pour justifier toutes sortes de systèmes d’apprentissage fédéré, de la prédiction des cas cliniques de COVID-19 à la détection de problèmes de santé mentale – le tout en s’appuyant sur des informations hautement sensibles qui, si elles étaient connues des proches ou des employeurs, pourraient bien bouleverser la vie des personnes souffrant de ces maladies.
https://www.nature.com/articles/s41591-021-01506-3 
https://www.nature.com/articles/s41746-021-00431-6
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9871861 

Et j’insiste sur le fait que les cas que je cite ici sont publiés dans des revues scientifiques à comité de lecture. Autrement dit, les auteurs et les revues scientifiques sont fières d’avoir reproduit les systèmes d’information proposés par Google et de les avoir appliqué au traitement de données personnelles. Pire, en célébrant ces travaux (ce qui conduira ensuite à des bourses et à des promotions pour les auteurs, au détriment d'autres), en célébrant ces travaux, la culture et la mésinformation au sein de la communauté scientifique valide, encourage et normalise le déploiement précipité et mal justifié de technologies dangereuses, voire le mépris pour la recherche rigoureuse sur la sécurité. 

Mais surtout, pendant ce temps, Google peut tranquillement déployer des algorithmes de langage beaucoup plus sophistiqués encore, et beaucoup plus capables de mémorisation, en s’appuyant sur les données des utilisateurs. Et quand on les poursuivra pour mise en danger des utilisateurs, ils pourront répondre à juste titre que les garanties de protection des informations sensibles de leurs algorithmes ont été scientifiquement prouvées et validées par la communauté scientifique, qui n'a cessé d'affirmer que l'apprentissage fédéré garantit la protection des informations sensibles. C’est terriblement dangereux.
https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/ 

### Mon travail de débunking scientifique
Et malheureusement, je peux témoigner à titre personnel de l’ampleur de la tâche qui consiste à débunker cette désinformation dans le milieu scientifique. J’ai eu l’occasion d’être revieweur dernièrement à ICML et à NeurIPS, les deux conférences les plus prestigieuses en machine learning. Et de façon très troublante, un article sur trois en apprentissage fédéré dont on m’a demandé de faire la revue propageait à son tour cette désinformation. J’ai dû ainsi prendre le temps de faire le débunking de cette désinformation — et c’est quand même troublant de devoir débunker une chose aussi triviale à des experts du domaine…

Mais surtout, avec six collègues, notamment El Mahdi El Mhamdi, lui-même chercheur chez Google, on a écrit un article qui synthétise les théorèmes mathématiques d’impossibilité sur la sécurité et la privacy des algorithmes très sophistiqués de machine learning. Combiner sécurité et performance est tout simplement impossible. Et donc, si on valorise vraiment la sécurité des milliards d'utilisateurs et de nos sociétés, il faut urgemment sacrifier la performance.
https://arxiv.org/abs/2209.15259 

Malheureusement, l’accueil de notre article de recherche par Google d’un côté, et par la communauté scientifique de l’autre, a été compliqué. Pour commencer, Google, dont il nous fallait l’approbation pour ajouter l’affiliation Google de Mahdi, a mis le papier en suspens pendant plusieurs mois, avant de s’opposer à la soumission — une affaire a priori inédite, un an après le licenciement de Timnit Gebru, même si l’issue était prévisible. D’ailleurs, on avait prévu le coup, et on a finalement soumis le papier avec l’autre affiliation de Mahdi, qui se trouve être aussi Professeur à l’École Polytechnique.

Mais de l’autre côté, il y a eu la revue par les pairs, plus décevante encore. Le principal critique, qui s’opposait fortement à la publication, a justifié le rejet de notre article en s’appuyant justement sur la désinformation selon laquelle l’absence d’enregistrement des données personnelles suffisait à garantir la protection des données personnelles, le tout dans un message très mal écrit et difficilement compréhensible. Une revue absolument scandaleuse…

“the data for autocompletion is not publicly recorded, eg. messages but public data that large language models are trained on, such as Wikipedia so these scen. [sic]” (Review FAccT)

La revue cite aussi l’utilisation majoritaire de Wikipedia, ce qui est une autre mésinformation, en tout cas si on considère les algorithmes les plus spectaculaires comme GPT-3 ou PaLM qui s'appuient beaucoup plus sur des conversations sur les réseaux sociaux, ou si on considère les algorithmes les plus influents comme les algorithmes de recommandation, Siri et les claviers intelligents, dont l’opacité permet même aux entreprises d’empiéter beaucoup plus encore sur les données personnelles.
https://arxiv.org/pdf/2204.02311.pdf

Si la version alors soumise de notre article était par ailleurs critiquable et critiquée par les autres revieweurs, la décision finale de rejet de notre article aura clairement été influencée par le principal critique qui, au mieux, propageait de la désinformation, au pire voulait empêcher la publication de critiques scientifiques de la sécurité du deep learning — d’expérience, comme tout le monde, certains chercheurs en machine learning détestent qu’on exige d’eux qu’ils mesurent les véritables conséquences sociales de leur travaux et de ceux de leurs collègues, et peuvent abuser de raisonnement motivé pour justifier l’éthique de leurs travaux payés par le contribuable ou des entreprises privées, en affirmant qu’ils ne font que des travaux théoriques sans application directe, si ce n’est mieux comprendre l’apprentissage, ou qu’il existe aussi des applications positives de leurs recherches, ou encore qu’ils ne sont pas responsables des mauvaises réutilisations de leurs pubilcations. Ainsi, pour défendre l’éthique de leurs travaux et pour ne pas avoir à questionner leur carrière, certains académiques préfèrent fermer l’oeil sur les millions de vies qu’ils contribuent à mettre en danger.
https://www.theguardian.com/technology/2021/apr/12/facebook-fake-engagement-whistleblower-sophie-zhang 

Depuis, notre article a été réécrit et resoumis en mode “Systematization of Knowledge”, c’est-à-dire “méta-analyse” en gros, avec beaucoup d’ajouts de références et beaucoup plus de pédagogie pour comprendre la portée des travaux en sécurité du machine learning, aujourd’hui encore beaucoup trop difficiles à publier dans les conférences prestigieuses et beaucoup trop souvent ignorées par les chercheurs en machine learning. On verra avec curiosité ce que les revieweurs en diront…
https://arxiv.org/abs/2209.15259 

### La confidentialité différentielle… et ses limites
Alors, s’il y a des experts en privacy parmi vous, vous vous écriez peut-être que je n’ai toujours pas mentionné le concept dominant dans la recherche académique en protection des informations sensibles, à savoir la confidentialité différentielle. Bon, la vidéo est déjà très longue, donc je ne vais pas pouvoir prendre le temps de bien expliquer de quoi il s’agit.

Mais en gros, l’idée de la confidentialité différentielle est d’envoyer des versions bruitées des objets calculés à partir des données personnelles aux serveurs de Google, de sorte à réduire la capacité de Google à connaître ces données personnelles. Et en effet, on a là une solution dont on peut prouver qu’elle protège en partie les données personnelles.

Mais j’insiste sur le fait, bien trop méconnu des chercheurs du domaine, que la confidentialité différentielle protège les données personnelles, c’est-à-dire les données explicitement associées à une personne, comme les données sur le téléphone de celle-ci, ou celles enregistrées dans son compte Google. Or il y a là une distinction cruciale à faire avec les informations sensibles, c’est-à-dire les informations que quelqu’un ne veut pas que les algorithmes diffusent. 

Surtout dans le cas du langage, des activités sur le web ou de la santé, où les données d’une personne contiennent des informations sensibles à propos d’autres personnes, la différence est majeure. Ma mère peut envoyer mon adresse à mon père. L’information sensible qu’est mon adresse serait alors dans les données personnelles de mes parents, et ne seraient pas protégées comme la confidentialité différentielle prévoit de les protéger. De la même manière, les données personnelles génomiques de mes parents trahissent inéluctablement des informations sensibles à mon sujet, puisque mon ADN est une combinaison des ADN de mes parents. Dès lors, mes informations sensibles peuvent être fuitées, même si mes données personnelles sont parfaitement protégées. Pour bien se rendre compte de l’ampleur de la fuite d’information via ces canaux indirects, je vous invite à voir cette vidéo, ou à lire l'histoire du Golden State Killer, qui a été trahi par les données personnelles de cousins éloignés !
https://tournesol.app/entities/yt:KT18KJouHWg 

Bref. La notion de confidentialité différentielle est en fait trompeuse pour la plupart des cas d’application à grand enjeu. Ceci dit, même à supposer que la confidentialité différentielle est un bon concept, sa mise en pratique est absolument catastrophique, comme en parle cet excellent article. Je ne rentre pas dans les détails, mais en gros, les algorithmes de géants de la tech n’ont pas du tout des garanties suffisantes, même lorsque ces derniers prétendent garantir la confidentialité différentielle.
https://dl.acm.org/doi/10.1145/3433638 

Et je ne parle pas de la réutilisation malsaine et dangereuse de la notion de differential privacy par les géants de la tech, notamment en termes d’ethics washing, d’abus de position dominante et de silenciations d’autres problèmes plus graves.
https://privacytools.seas.harvard.edu/publications/algorithmic-institutional-logics-politics-differential-privacy 

Et pourtant, aujourd’hui, la confidentialité différentielle est vue comme le Saint-Graal de la protection des informations sensibles par une écrasante majorité de la communauté scientifique, si bien qu’un algorithme qui garantit cette propriété est souvent considéré “privacy-preserving”.

### Désinformation sur la confidentialité différentielle
En 2021, un article a été accepté à ICLR, l’une des trois plus prestigieuses conférences en machine learning aux côtés d’ICML et NeurIPS, dont le titre affirme que “les grands modèles de langage peuvent être des systèmes d'apprentissage fortement différentiellement confidentiels”. Ah…. quel titre dangereusement trompeur ! Et encore une fois, l’un des auteurs de cette désinformation scientifique bosse chez Google.
https://openreview.net/forum?id=bVuP3ltATMz 

Alors, si on cherche l’interprétation charitable de ce titre, en un sens, ok, oui, ces algorithmes de langages peuvent être des “apprentis fortement différentiellement privés”, dans le sens où, après avoir été conçus et entraînés dans un premier temps, ils peuvent dans un second temps continuer à apprendre en respectant la privacy des données exploitées dans ce second temps. Et c’est ce que fait le papier… sauf que. 

Sauf que la performance apprise dans ce second temps est en gros triviale par rapport à ce qui était initialement appris. De plus, le papier ne parle absolument pas de protection d’informations sensibles dans la première phase d’apprentissage, qui est bien la phase critique car elle nécessite beaucoup plus de données, et où l'algorithme va apprendre le plus d'informations sensibles.

Mais surtout, le titre peut facilement donner l’impression que les algorithmes de langage peuvent garantir la confidentialité différentielle de façon générale, tout en ayant d’excellentes performances. C’est sans doute la mésinformation qui va se répandre dans la communauté scientifique exposée à cet article et qui, trop souvent, s’arrête au titre et au résumé, ne prend pas le temps de lire le contenu de l’article et, surtout, fait confiance aux auteurs et à la revue par les pairs…

Publier un article qui, clairement, va laisser entendre que le deep learning peut facilement protéger les informations sensibles sans perte de performance, c’est, je pense, une grave désinformation scientifique. Une désinformation sans fake news, car techniquement, on peut toujours justifier le titre de l'article. Mais ça me semble néanmoins bien être une désinformation, c’est-à-dire une façon intentionnelle de nuire à la propagation d’informations de qualité, en l'occurrence en faisant croire que les problèmes de privacy peuvent facilement être réglés, à l’instar d’un industriel polluant qui laisse croire qu’il suffit de planter des arbres pour combattre le changement climatique. Cette forme de désinformation me semble d’ailleurs beaucoup plus dangereuse.
https://tournesol.app/entities/yt:l2j8IaLx0Yk 
https://tournesol.app/entities/yt:Y4yeTTOTfO8 

Mais soyons clair. Non, les très gros modèles de machine learning ne peuvent pas atteindre leur performance tout en protégeant les informations sensibles, en tout cas avec les algorithmes d’aujourd’hui. Pire, de nombreux travaux théoriques suggèrent au contraire l’impossibilité d’une telle protection des informations sensibles sans dégradation majeure des performances, comme on en parle dans notre article. La course à la performance, dans laquelle nous sommes complètement engagés à coups de communiqués de presse sur Dall-E et LaMDA, et massivement relayés par les médias et les utilisateurs des réseaux sociaux, nuira inéluctablement grandement à la privacy.
https://arxiv.org/abs/2209.15259 

## La sécurité des algorithmes d’apprentissage
### Les données manipulent les algorithmes

Les algorithmes de machine learning sont donc une grave menace pour la protection des informations sensibles. Et pourtant, cette menace me semble presque dérisoire par rapport à ce qui me préoccupe vraiment. Ce qui me terrifie, c’est surtout que les algorithmes amplifient, encouragent et normalisent la désinformation, le cyber-harcèlement et les appels à la haine. Comme on l’a vu en Chine, en Inde, en Éthiopie, au Royaume-Uni, au Brésil ou encore aux États-Unis, cette désinformation et cette haine peuvent conduire à des politiques dangereuses, à des émeutes meurtrières, voire à des génocides de populations entières. 
https://thediplomat.com/2022/09/chinas-changing-disinformation-and-propaganda-targeting-taiwan/ 
https://www.nytimes.com/2022/02/08/world/asia/india-hate-speech-muslims.html
https://edition.cnn.com/2021/10/25/business/ethiopia-violence-facebook-papers-cmd-intl/index.html 
https://www.npr.org/sections/parallels/2016/06/29/484038396/after-brexit-vote-u-k-sees-a-wave-of-hate-crimes-and-racist-abuse
https://insights.bu.edu/when-influence-goes-too-far-social-medias-effect-on-the-capitol-riots/ 
https://blog.ethiopianeurosurgery.com/the-unhindered-use-of-social-media-to-promote-tigray-genocide-continues 

Après l’ONU, Amnesty International a reconnu le rôle criminel de Facebook dans l’amplification à la haine au Myanmar contre la communauté des Rohingyas, et exige réparation. Les Rohingyas ont d’ailleurs entamé des procédures de justice, et exigent 150 milliards de dollars de réparation — ce qui, avec la chute du cours de Facebook, représente maintenant près de la moitié de la valeur de l’entreprise !
https://www.amnesty.org/en/latest/news/2022/09/myanmar-facebooks-systems-promoted-violence-against-rohingya-meta-owes-reparations-new-report/ 
https://www.bbc.com/news/world-asia-59558090 
https://www.cnbc.com/2022/09/30/facebook-scrambles-to-escape-death-spiral-as-users-flee-sales-drop.html 

Mais pourquoi les algorithmes d’apprentissage favorisent-ils ainsi la désinformation et la haine ? Sont-ils par nature malveillants ? Leurs concepteurs cherchent-ils à propager la désinformation et la haine ? Non. Sans doute pas. En tout cas pas intentionnellement — quoique, ils voient clairement que ceci augmente leurs revenus et leurs bonus.
https://tournesol.app/entities/yt:sAjm3-IaRtI 

Le problème, c’est surtout que les algorithmes d’apprentissage modernes sont généralement conçus pour apprendre et généraliser les données dont ils apprennent. Si leurs données contiennent de la désinformation, du harcèlement et de la haine, alors ces algorithmes vont répéter, généraliser et amplifier la désinformation, le harcèlement et la haine. Comme on en a déjà parlé, les données manipulent les algorithmes d’apprentissage.
https://tournesol.app/entities/yt:vYb3rB0jU70

### Se protéger avec des “nouveaux algorithmes” ?
OK, mais ne serait-il pas possible de se protéger contre les comptes malveillants qui produisent ou amplifient la désinformation ? Eh bien, en 2021, une réponse positive est fournie par quatre chercheurs, dont l’un travaille à Google… oui vous l’aurez compris, cet article accepté par les pairs académiques dans la très prestigieuse conférence “Economics and Computing”, est encore une fois, je pense, une énorme désinformation. 

Dans la dernière phrase de son résumé, l’article affirme : “nos résultats indiquent que les plateformes en ligne peuvent efficacement combattre les utilisateurs frauduleux à faible coût, en concevant de nouveaux algorithmes d’apprentissage qui garantissent la convergence efficace, même lorsque la plateforme est complètement ignorante du nombre et de l’identité des faux utilisateurs”. L’analogie avec la désinformation des entreprises pétrolières qui prétendent que les problèmes climatiques peuvent être efficacement combattu par des “nouvelles technologies” est tellement frappante, que j’ai vraiment du mal à ne pas y voir de malveillance.
https://dl.acm.org/doi/10.1145/3465456.3467580 

“our results indicate that online platforms can effectively combat fraudulent users without incurring large costs by designing new learning algorithms that guarantee efficient convergence even when the platform is completely oblivious to the number and identity of the fake users.”

Encore une fois, il suffit de se plonger dans l’article pour se rendre compte que cette phrase n’est en fait valide que sous une hypothèse complètement irréaliste. Mais pour s’en rendre compte, vu comment le titre et le résumé ont été rédigés, il est malheureusement nécessaire de se plonger dans l’article, et de prendre le temps d’analyser les théorèmes — ce qui clairement demande une certaine aisance mathématique. Mais une fois ce travail effectué, on s’en rend compte : l’affirmation n’est valide que si le nombre de faux comptes est exponentiellement plus faible que le nombre de comptes authentiques. Exponentiellement plus faible.

Autrement dit, s’il y a 30 faux comptes, alors vous pourrez combattre ces faux comptes à faible coût avec de “nouveaux algorithmes” uniquement si votre plateforme possède, disons 2^30 comptes authentiques sur votre plateforme, soit un milliards d’utilisateurs humains ! Et pour peu que les attaquants créent 3 faux comptes de plus, il vous faudra plus de vrais comptes qu’il n’y a d’humains sur terre !

Mais bien sûr l’hypothèse selon laquelle Google ne disposerait que de 33 faux comptes est elle-même complètement irréaliste ! Souvenez-vous de cette statistique : chaque année Facebook retire autour de 7 milliards de faux comptes. Dans ce contexte, non seulement l’affirmation principale de l’article est complètement trompeuse, mais l’article dans sa globalité est lui-même complètement inutile. Et pourtant, le papier a été accepté à la conférence Economics & Computing, cette même conférence qui a rejeté un de nos papiers en 2022 malgré un revieweur enthousiaste, parce que l’autre revieweur juge que “le sujet — malgré les affirmations de liens avec le Machine Learning — ne semble pas important” — ce qui me permet de souligner le fait que l’acceptation à publication scientifique est beaucoup plus liée à l’attractivité, voire au sensationnalisme, que ce qu’on pourrait croire, et n’a en tout cas rien “d’objectif”, de “neutre” ou “d’impartial”, ce qui se rend le processus scientifique de revue par les pairs très vulnérable à désinformation, surtout quand on sait que beaucoup de revieweurs sont des employés de Google ou des chercheurs financés par Google…
https://arxiv.org/abs/2106.02394 

“the topic -- despite the claims of deep ML connections -- does not seem significant”
La même année, avec des collègues, notamment l’excellent Sadegh Farhadkhani, on a publié à NeurIPS, l’une des tops conférences en machine learning, qui, au contraire, démontre l’impossibilité mathématique de se défendre efficacement face à des faux comptes indiscernables des vrais comptes, surtout lorsqu’il y a beaucoup d’hétérogénéité parmi les comptes authentiques. Même s’il n’a pas été écrit avec ce but, notre article débunke la désinformation citée plus haut. Mais malheureusement, pour l’instant, le débunking fait moins de citations scientifiques (8) que la désinformation (14).
https://proceedings.neurips.cc/paper/2021/hash/d2cd33e9c0236a8c2d8bd3fa91ad3acf-Abstract.html 

La communauté scientifique propage beaucoup plus l’affirmation complètement trompeuse selon laquelle la résilience aux faux comptes peut se régler facilement par de “nouveaux algorithmes”, que les arguments beaucoup plus solides sur la vulnérabilité des algorithmes à grande échelle. Et encore une fois, l’enjeu est majeur, puisqu’il permet à Google de prétendre qu’ils maîtrisent parfaitement leurs technologies, à l’instar de l’entreprise privée du film #DontLookUp qui prétend pouvoir miner et détourner seule un astéroïde. 
https://tournesol.app/entities/yt:lqfNjyuMKeY 

De plus, fort de cet article accepté par la communauté scientifique, Google peut alors affirmer aux législateurs que des audits ou des régulations de cybersécurité sont superflues, voire contre-productives pour la société civile, puisqu'il est "scientifiquement établi" qu'il leur suffit de développer de nouveaux algorithmes, comme eux seuls savent le faire.

Et pourtant, pendant ce temps, le besoin et la capacité à influencer les algorithmes de recommandation semblent être devenus une évidence pour beaucoup parmi vous qui me suivez, comme j’ai pu le voir dans mes commentaires. 

Et du coup, il faut s’attendre à ceux que des entités puissantes et malveillantes, comme des gouvernements autoritaires ou des multinationales qui ont des profits à protéger, exploitent eux aussi cette fonctionnalité du machine learning pour biaiser les algorithmes les plus influents du web. Mais comme ils ont beaucoup plus de moyens pour y parvenir que ceux qui militent pour une information de qualité, il faut s’attendre à ce que les algorithmes de machine learning soient beaucoup plus influencés par la désinformation que par l’information de qualité. C'est bien ce que ne cesse de montrer la recherche, le journalisme et les leaks des géants de la tech.
https://ieeexplore.ieee.org/document/9581221 
https://www.nytimes.com/2021/01/29/technology/commercial-disinformation-huawei-belgium.html
https://tournesol.app/entities/yt:sAjm3-IaRtI 

### La dangereuse hypothèse politiquement biaisée qu'il faut généraliser les données
Malheureusement, les articles qui envisagent la possibilité de manipulation des algorithmes sont encore incroyablement rares dans les conférences en machine learning. De manière absolument terrifiante, la quasi-totalité de la recherche en machine learning fait l’hypothèse dangereusement irréaliste que les algorithmes d’apprentissage devraient avoir pour but d’apprendre et de généraliser toutes les données auxquelles ils ont accès — avec souvent derrière la fameuse hypothèse i.i.d. sur laquelle la quasi-entièreté de la “théorie du machine learning” s’appuie, et qui suppose que toutes les données sont tirées indépendamment d’une mystérieuse vraie loi de probabilité qu'il faut absolument apprendre — une sorte de main invisible de l’intelligence artificielle.
https://tournesol.app/entities/yt:Olur9mRLNjM  

Vraiment, cette hypothèse, ok, ça fait des jolies mathématiques, et ça permet aux chercheurs de briller par leur ingéniosité technique. Mais c’est vraiment comme faire l’hypothèse des marchés efficients et d’appliquer ça à des marchés comme celui de l’information, celui de l’électricité ou celui des ressources naturelles. OK, le chercheur se démarque par sa virtuose technique, et ça va l’aider à gagner des prix, à avoir son job de rêve et parfois gagner des millions s’il flirte avec Google notamment ; mais ce faisant, il propage aussi des mésinformations et des habitudes extrêmement dangereuses pour les sociétés humaines.
https://tournesol.app/entities/yt:or5WdufFrmI 
https://tournesol.app/entities/yt:ienoSbONyhw 

Le problème est pire encore dans le cadre de l’apprentissage fédéré, puisque les participants au système peuvent plus facilement envoyer n’importe quoi aux serveurs de Google, qui pourront plus difficilement déterminer si les gradients envoyés correspondent à des données plausibles. De plus, ces participants pourront plus facilement optimiser leurs attaques pour encore mieux biaiser les algorithmes que tout le monde utilisera dans les directions souhaitées. L’écrasante majorité des algorithmes d’apprentissage fédéré publiés dans la littérature scientifique est ainsi ultra vulnérable à la première attaque dite Byzantine.
https://tournesol.app/entities/yt:baxj1kuaWJ8 

Depuis 2017, pour enfin considérer la sécurité du machine learning au moment de l’apprentissage fédéré, El Mahdi El Mhamdi, alors en thèse à l’EPFL, et ses collègues, notamment Peva Blanchard et Julien Stainer, ont inventé le concept d’apprentissage résilient aux Byzantins, avec notamment une publication à NeurIPS. Une longue série d’articles a suivi, avec certains où j’ai été impliqué, publiés dans les meilleures conférences, notamment ICML, ICLR et MLSys, pour mieux comprendre comment sécuriser l’apprentissage fédéré, mais aussi pour identifier les limites théoriques à la sécurisation de tels systèmes.

Et dès 2018, Mahdi motivait ses travaux par la crainte de la montée des mouvements anti-vax et de l’extrémisme via les algorithmes de recommandation — bon, ça ne surprendra pas ceux parmi vous qui ont lu notre livre #LeFabuleuxChantier, dont l’écriture a aussi débuté en 2018, et qui considère que rendre ces algorithmes robustement bénéfiques est le plus urgent et le plus fabuleux des chantiers pour l’humanité. À notre grande frustration, malgré le chaos informationnel du COVID, les émeutes du Capitol et la guerre en Ukraine, la communauté scientifique en machine learning peine encore gravement à prioriser les problèmes de sécurité. Et Google n’y est pas pour rien ! 
https://www.fr.fnac.ch/a13596954/Le-Nguyen-Hoang-Le-fabuleux-chantier 

### La robustesse “pratique” selon Google
Clairement, la recherche qui révèle les vulnérabilités de sécurité de l’apprentissage fédéré est très gênante pour des entreprises comme Google, qui y voyait l’avenir de leur business de publicités ciblées et de recommandation de contenus. Et c’est sans doute ça qui les a motivés à encourager et financer une recherche qui vise clairement à dénigrer la recherche sur la sécurité. Et à ma grande déception, l’article de désinformation ainsi produit, dont l’un des auteurs est un chercheur de Google, a été accepté par la communauté scientifique, dans la revue IEEE Security & Privacy.
https://ieeexplore.ieee.org/document/9833647 

Je cite le résumé de l’article : “Nous présentons une analyse critique des attaques [...] dans des environnements d’apprentissage fédéré pratiques et de production [...] Nos résultats sont plutôt surprenants : contrairement à la croyance établie, nous montrons que l’apprentissage fédéré est très robuste en pratique, même en utilisant des défenses simples et peu coûteuses.” Vues les nombreuses formulations trompeuses de ces phrases, il ne fait essentiellement aucun doute à mes yeux qu’il s’agit là de désinformation intentionnelle.

“We present a critical analysis of untargeted poisoning attacks under practical, production FL environments by carefully characterizing the set of realistic threat models and adversarial capabilities. Our findings are rather surprising: contrary to the established belief, we show that FL is highly robust in practice even when using simple, low-cost defenses”

En bref, le point le plus important à retenir pour débunker cette désinformation, c’est que l’environnement “pratique et de production” considéré par cet article n’est absolument pas l’environnement “pratique et de production” des algorithmes déployés à très grande échelle et avec des enjeux géopolitiques majeurs. En effet, l’article étudie exclusivement la classification d’images pour deux jeux de données standards, à savoir MNIST et CIFAR10 — typiquement reconnaître que ceci est un 3, ou que ceci est un camion. 

Ce qu’il faut savoir, c’est que ces tâches sont extrêmement faciles, surtout en comparaison de ce qu’on exige des algorithmes de langage ou de recommandation. Mais alors, la robustesse dans ces cas simples ne dit en fait presque rien de la robustesse dans les vrais cas d’applications beaucoup plus complexe. En particulier, il me semble dangereusement mensonger de prétendre que l’article en question correspond à la “pratique” ou à l’environnement “de production”.

C’est exactement comme si, pour valider une voiture autonome, on la testait sur un circuit fermé et on affirmait : “contrairement à la croyance établie, nous montrons que la voiture autonome est très robuste en pratique, même en utilisant des algorithmes simples et peu coûteux”. Affirmer cela, pour des applications aussi dangereuses que les algorithmes utilisés à grande échelle, et donc beaucoup plus dangereux encore que des voitures autonomes puisqu'ils mettent déjà des millions de vies en danger, en Inde, au Brésil ou en Éthiopie, ça me semble criminel. 

Bref. Contrairement à ce que fait cet article, la sécurité ne peut jamais se valider à partir d’exemples où tout va bien, surtout quand ces exemples ne sont pas représentatif de l’énorme diversité des cas d’applications et des attaques que rencontreront les systèmes qu’on veut sécuriser. La sécurité exige des garanties contre toutes sortes d’attaques, y compris celles auxquelles on n’a pas encore eu l’intelligence de penser. Affirmer une robustesse sans garantie de la sortie, c’est au mieux dangereusement trompeur, et au pire, aussi mensonger que de nier les risques d’amplification massive de la haine pouvant aller jusqu’au génocide des algorithmes modernes.

Et autant, ok, Google est une multinationale, et il faut attendre d’elles qu’elles réutilisent les méthodes de marchands de doute de Total et de Philip Morris. Là où je suis extrêmement déçu, c’est davantage vis-à-vis de la communauté scientifique, qui non seulement gobe et valide scientifiquement cette désinformation dans des revues scientifiques prestigieuses, mais va même souvent jusqu’à la célébrer en invitant les auteurs de tels articles à prendre la parole dans des universités. J’aimerais que les chercheurs soient beaucoup plus méfiants de leurs collègues des GAFAM qu’ils invitent, et beaucoup plus enclins à leur poser des questions difficiles, comme au sujet de la transparence, de la sécurité et de l’éthique des algorithmes que leurs employeurs développent, voire de l'origine souvent questionnable de leurs propres salaires.

### Notre débunking fait toujours moins de citations…
Cette fois, vu la gravité de ces affirmations, avec Sadegh Farhadkhani et Oscar Villemaud notamment, on a pris le temps et fait l’effort de débunker cet article et toutes les mésinformations autour de la sécurité de l’apprentissage fédéré, dans un article qui a récemment été accepté à ICML, l’une des tops conférences en machine learning — je dis ça comme si c’était facile de publier à ICML, mais c’est vraiment un travail monumental à abattre, qui fait rêver plus d’un académique, voire qui fait rêver des entreprises… 
https://proceedings.mlr.press/v162/farhadkhani22b.html 

[Techniquement, je réécris un peu l’histoire, car l’histoire de cet article est plus compliquée, et ne s’est transformée en débunking qu’en route, au moment où on a découvert l’article de Google.]

En gros, l’article dont on a parlé plus tôt affirme que les attaques par injection de données sont bénignes, alors que les attaques Byzantines sont irréalistes. Notre article débunke ces affirmations en démontrant que, dans les cadres théoriques prédominants de l’apprentissage fédéré personnalisé, ces deux attaques sont en fait équivalentes. Si l’une est bénigne, l’autre doit nécessairement être bénigne aussi. Et si l’une est réaliste, l’autre doit nécessairement être considérée réaliste elle aussi.

Notre article exploite ensuite cette équivalence pour concevoir des attaques par injection de données dévastatrices contre l’apprentissage fédéré non sécurisé, y compris dans des cas simplistes comme la classification d’images MNIST. Mais surtout, en démontrant l’équivalence, notre article permet d’importer des théorèmes issus de la littérature sur la résilience byzantine au cas de la résilience à l’injection de données malveillantes. En particulier, nous démontrons l’impossibilité de sécuriser tout apprentissage fédéré, dans le cas où il y a une forte hétérogénéité dans le comportement des utilisateurs authentiques.
https://slideslive.com/38983859 

Malheureusement, encore une fois, la désinformation de Google a beaucoup plus de citations que notre article, en l’occurence 26 fois plus, puisque notre article n’a été cité qu’une seule fois pour l’instant. Et j’aimerais pouvoir vous dire que ça va changer… mais bon, je ne suis pas optimiste.

### Généraliser les données, une hypothèse politiquement (et financièrement) motivée ?
Ceci étant dit, comme nous le soulignons dans l’article de synthèse de la littérature, tout le travail qu’on a effectué sur la sécurité du machine learning depuis maintenant 6 ans reste à prendre avec des pincettes, surtout lorsqu’il s’agit des théorèmes positifs de sécurité. En effet, ces théorèmes positifs de sécurité reposent eux-mêmes sur des hypothèses peu réalistes ou très problématiques, comme le fait qu’il faut généraliser le comportement de la majorité des utilisateurs.
https://arxiv.org/abs/2209.15259 

Alors, bien sûr, il y a des cas d’application où généraliser les données est justifiable, notamment lorsqu’on applique le machine learning à la physique ou la chimie. Cependant, cette hypothèse est devenue si commune et si répandue que, très souvent, les scientifiques n’attendent plus de leurs collègues qu’ils la justifient. Dès lors, le champ d’applicabilité des travaux publiés est flou et inexpliqué dans une énorme proportion de la recherche académique.

Or, dans les applications pratiques les plus influentes, les plus lucratives et les plus dangereuses du machine learning, notamment le ciblage publicitaire et la recommandation de contenus, généraliser les données revient à normaliser le statu quo, à amplifier les addictions ou à faire triompher la désinformation, ce qui me semble terriblement dangereux pour les sociétés humaines.
https://tournesol.app/entities/yt:lYXQvHhfKuM

Et le problème n’est pas limité à des histoires de faux comptes ! Même en supposant l’absence de tout faux comptes, ce qui est complètement irréaliste mais soyons charitables un instant, le comportement de beaucoup de comptes authentiques peut être très indésirable à apprendre et à généraliser, comme on l’a vu par exemple dans le cas de Tay Tweets. Cette IA de Microsoft a appris des comptes trolls, mais probablement authentiques. Elle a du coup généralisé leurs manières de s’exprimer, ce qui l’a transformée en monstre sexiste, raciste et génocidaire… 
https://tournesol.app/entities/yt:vYb3rB0jU70 

La désinformation, le harcèlement et la haine sont en fait massivement produits par une énorme fraction, peut-être même par la majorité, des comptes authentiques — surtout sur des plateformes comme Twitter ou Reddit. Toute IA qui apprendra de ces données, même avec les meilleures procédures de sécurité proposées jusque là dans la littérature scientifique, sera vouée à répéter et amplifier massivement beaucoup de comportements non seulement détestables, mais qui pourraient aussi et surtout aggraver le déclin démocratique à l’échelle mondiale, et les risques de guerre mondiale.
https://www.v-dem.net/democracy_reports.html 
https://tournesol.app/entities/yt:MV6a-AkiKgQ 

Il est urgent que toute la communauté du machine learning mesure réellement l’ampleur de la responsabilité que l’impact de leur travail implique, et des conséquences dangereuses de leur silence. Depuis que Google utilise nos algorithmes à l’échelle planétaire, nous ne sommes plus des chercheurs dont la science fondamentale est déconnectée du monde réel. Nos choix de sujets de recherche et nos hypothèses de travail, mais aussi nos décisions sur les collaborations à établir, les intervenants à inviter et les collègues à recruter, tout ça joue désormais un rôle majeur dans la guerre de l’information sur la sécurité des algorithmes les plus influents du monde moderne, et sur l’urgence du besoin d’investir massivement dans leurs audits.
https://tournesol.app/entities/yt:lYXQvHhfKuM 

À chaque fois que nous faisons l’hypothèse qu’il faut minimiser la fonction de perte qui cherche à coller aux données, surtout sans souligner les conséquences d’invoquer ces hypothèse sur l’inapplicabilité de nos travaux notamment pour les algorithmes les plus influents, nous célébrons et nous normalisons l’apprentissage et la généralisation des données, et nous contribuons à omettre et à ignorer les problèmes pourtant largement avérés de cyber-harcèlement au moment de la conception des algorithmes.

À chaque fois que nous ne questionnons pas cette hypothèse dans les publications et dans les présentations des autres, nous normalisons et nous facilitons le déploiement d’algorithmes qui apprennent, généralisent et amplifient massivement et dangereusement la désinformation et les appels à la haine.

Et à chaque fois que nous relayons des images produites par des algorithmes de Facebook, des textes produits par GPT-3 ou des avancées biologiques de Google DeepMind, nous amplifions la publicité gratuite pour la spectularité du machine learning, et surtout, nous faisons toujours plus de la sécurité et de l’éthique des algorithmes une #MuteNews, c'est-à-dire un sujet dont personne ne parle, en plus de normaliser l’absence de contrôle démocratique sur les algorithmes qui contrôlent le plus le flux de l’information et de la désinformation à travers le monde.
https://www.youtube.com/watch?v=XW_nO2NMH_g

Si vous pensez qu'il n'y a rien de mal à célébrer les avancées spectaculaires du machine learning, c'est certainement parce que vous n'avez jamais essayé à promouvoir activement la sécurité et l'éthique du machine learning, et que vous ne vous êtes ainsi jamais rendu compte de l'ampleur de l'inattention, voire du mépris, que suscitent ces sujets pourtant beaucoup plus importants. Célébrer la spectacularité du machine learning dans le contexte actuel, c’est un peu comme célébrer l'ingéniosité de la construction spectaculaire de stades climatisés en plein désert. C’est prioriser l’avancée technologique, et le confort du métier de chercheur en machine learning, aux dépens des souffrances avérées et dramatiques de millions d’humains.
https://tournesol.app/entities/yt:1t6hoJNXugM 

## Conclusion
### L’éthique et la sécurité ont urgemment besoin de financements massifs
J’espère vous avoir convaincu que des géants comme Google se sont bel et bien lancés dans la désinformation scientifique, et que celle-ci est extrêmement dangereuse, surtout dans le contexte de cyber-guerre et d’info-guerre du monde moderne.
https://tournesol.app/entities/yt:U_7CGl6VWaQ 

Et malheureusement, la communauté scientifique est en train de gober, d’amplifier et de normaliser la désinformation de Google. Mais surtout, les rares à vraiment prendre le temps d’identifier cette désinformation et de la déconstruire sont sous-valorisés, sous-financés, voire ignorés. Voilà qui semble garantir le triomphe désinformationnel et dystopique de Google, dans une guerre de l’information dans laquelle tous les chercheurs en informatique sont impliqués, qu’ils le veuillent ou non, ne serait-ce que parce qu’ils exigent de l’attention, des collaborateurs et du financement, au détriment de ceux qui luttent vraiment contre la désinformation — beaucoup de ressources que j'avais à ma disposition, notamment des collaborateurs de premier rang, ont ainsi été "volées" par d'autres académiques, qui les utilisent maintenant pour des travaux qui me semblent beaucoup moins utiles, voire parfois néfastes, à la société.
https://www.lemonde.fr/idees/article/2020/12/17/les-preoccupations-sur-l-ethique-des-algorithmes-doivent-etre-prises-tres-au-serieux_6063675_3232.html 
https://twitter.com/Forum_RTS/status/1569638286506737665 

Oui, parce que, pour débunker les travaux de Google comme je l’ai fait, surtout au niveau de la communauté scientifique, il faut vraiment beaucoup d’expertises, de temps et de sacrifices. Je sais que je me suis grillé vis-à-vis des entreprises qui se sont pleinement engagées dans la désinformation et ont littéralement perdu toute éthique. Mais je crains aussi que même des universités ou des chercheurs, qui peuvent avoir des partenariats très forts avec Google, puissent refuser de s’associer à moi à l’avenir. 
https://tournesol.app/entities/yt:HbFadtOxs4k 

Et alors, moi, j’ai d’autres projets, notamment de startups pour fournir de vraies solutions de sécurité et pour former des équipes de datasciences à la sécurité du machine learning, pour au hasard, éviter d’énormes fuites de données via les algorithmes de machine learning. Mais le problème est souvent plus délicat pour mes indispensables collaborateurs, généralement bien plus jeunes et moins établis, qui cherchent des stages, des bourses et des opportunités de carrière. Il m’arrive ainsi même de conseiller à des jeunes de “faire ce qu’il faut pour publier” — comme motiver le machine learning par d’hypothétiques applications en santé, alors que, clairement, il sera beaucoup plus probablement utilisé pour du ciblage publicitaire, ou comme motiver la recherche sur l’accélération des algorithmes par une réduction de consommation énergétique, alors que, clairement, il y aura un effet rebond inéluctable et que, surtout, ça va encore conduire à faire de l’éthique et de la sécurité un sujet secondaire, voire une #mutenews. 
https://tournesol.app/entities/yt:lYXQvHhfKuM 

Malheureusement, le monde académique, gouverné par le prestige et des métriques comme le h-index, en vient souvent à ignorer l’impact sociétal des travaux, et à tuer la carrière de ceux qui ne font pas ce qu’il faut pour publier. Publish or perish, comme on le dit souvent. Mais du coup, les survivants, ceux qui gouvernent la recherche académique, ceux qui décident des recrutements et des financements, seront plus souvent ceux qui ont fait davantage ce qu'il fallait pour publier, en surfant sur les tendances du moment, souvent initiées par Google avec des objectifs lucratifs ou de diversion, quitte à ignorer les sujets plus difficilement publiables et beaucoup moins cités par les pairs, comme la sécurité et l'éthique.

Pire encore, ces scientifiques établis ont trop souvent encore ce préjugé tenace selon lequel un “bon” scientifique ne doit s’exprimer que sur de la “science”, laquelle est “neutre” et “impartiale” — alors que, normaliser le fait de négliger la sécurité en faisant des hypothèses i.i.d., ou supposer que les données doivent être généralisées, ce n’est en fait absolument pas neutre ni impartial, et encore moins responsable ni scientifiquement correct. En tout cas, aussi triste que cela puisse paraître, on en est aujourd’hui là, dans un état du monde académique que la désinformation de Google, mais aussi de Facebook, OpenAI et autres Amazon, a sans doute cherché et a clairement réussi à obtenir.

À l’instar des algorithmes, tant que les financements et les promotions dans le monde académique ne tiendront pas sérieusement compte de l’impact social des travaux effectués, la science aura davantage la voix de ceux qui ont le plus de moyens pour écrire des articles d’apparence scientifique. Et il faut craindre que la théorie du machine learning prédominante soit celle qui arrange Google — à savoir une théorie qui ne suppose pas de dangerosité dans la généralisation et l’amplification des données, ou qui ne garantit la sécurité que sous des hypothèses irréalistes ou dans des expériences très restreintes.

### La recherche éthique : Tournesol
Et alors, débunker c’est bien, mais construire des solutions vraiment sécurisées, notamment pour la solidité des démocraties et pour la paix dans le monde, c’est bien sûr beaucoup mieux. Idéalement, j’aimerais pouvoir consacrer l’entièreté de mon temps à cela, plutôt qu’à passer mon temps à lire les tissus de mensonge que Google écrit pour ensuite expliquer à la communauté scientifique et au grand public le danger que ces torchons représentent. Mais bon, comme presque personne ne fait ce travail de débunking, et qu’il est quand même important qu’il soit fait… eh bien je le fais.

Mais bien sûr, ce qui m’anime le plus, ce qui me motive le plus, c’est davantage de construire des solutions pour notamment gouverner collaborativement et de manière sécurisée les algorithmes. En particulier, j’aimerais développer des solutions techniques pour garantir autant que possible que ces algorithmes fassent ce que nous voudrions vraiment qu’ils fassent, et non pas ce que des données incontrôlées du web les poussent à faire.

Et après maintenant 6 ans de lecture et de réflexion sur ce sujet précis, et 12 ans de réflexion plus globale sur la gouvernance collaborative, j’en suis venu à la conclusion qu’un projet participatif comme Tournesol était de très loin le projet le plus prometteur pour combattre la désinformation et la haine, et pour valoriser beaucoup plus l’information de qualité.
https://tournesol.app 

Au lieu de faire l’hypothèse i.i.d., Tournesol suppose que chaque utilisateur a ses propres préférences, non seulement sur les recommandations qu’il est amené à recevoir, mais aussi et surtout sur celles que les autres utilisateurs seront amenés à recevoir. Tournesol utilise ensuite l’état de l’art en machine learning sécurisé qu’on a développé depuis 6 ans, et en particulier un algorithme appelé Mehestan, pour agréger de manière équitable et sécurisée, les jugements de nos différents contributeurs.
https://arxiv.org/abs/2202.08656 

Ceci a soulevé de nombreux défis de recherche, qui vont du développement de nouveaux algorithmes et de la démonstration mathématique de leur sécurité, à l’analyse empirique des données collectées, en passant par une étude de l’impact social de Tournesol sur les habitudes de consommation des utilisateurs ; une étude qu’on est en train de mettre en place, et qui, si tout va bien, sera effectuée prochainement !
https://arxiv.org/abs/2107.07334 

Mais pour faire plus, et il nous faut faire beaucoup plus, Tournesol aurait surtout urgemment besoin de beaucoup plus de moyens, que les dons de quelques personnes pour payer notre unique employé, responsable du développement informatique du site web. Malheureusement, jusqu'à présent, nos 6 propositions de subventions ont toutes été rejetées, par diverses entités, qui me semblent toutes gravement sous-estimer l’impact monumental des algorithmes de recommandation et l’importance de concevoir une solution pour les gouverner collaborativement. Dans ces conditions, difficile de combattre Google.
https://tournesol.app/about/donate 

### Aidez-nous à rendre la recherche éthique !
L’un des principaux objectifs de Tournesol, c’est d’attirer l’attention de la communauté scientifique sur le problème de la gouvernance collaborative et sécurisée des algorithmes, en identifiant clairement des problèmes concrets dont la résolution aurait des réels impacts bénéfiques. Donc, si vous avez des amis dans le monde académique, envoyez-leur cette vidéo, et engagez la discussion avec eux sur la désinformation dont leur milieu souffre, et demandez-leur s'ils ont déjà vraiment réfléchi à la sécurité et à l'éthique des algorithmes les plus influents du monde. Et s'ils cherchent des sujets de recherche éthiques, mentionnez Tournesol.
https://arxiv.org/abs/2107.07334 

En particulier, Tournesol invite tout académique à télécharger la base de données publiques des jugements des contributeurs. Cette base de données contient maintenant presque 50 000 comparaisons, ce qui en fait une base de données comparables à MNIST ou CIFAR10, qu’on a mentionnées plus tôt. Mais ça serait vraiment bien qu’on dépasse la barre symbolique des 100 000 comparaisons, et vraiment convaincant si on dépassait celle des 1 million de comparaisons.

D’autant que cette base de données nous semble beaucoup plus intéressante que MNIST ou CIFAR10, puisqu’il s’agit de jugements humains sur ce que devraient faire les algorithmes les plus influents du monde moderne. Qui plus est, la structure de ces données ressemble beaucoup plus à la structure des données collectées par les géants du web, à savoir des données associées à des utilisateurs, soulevant ainsi naturellement les problématiques de privacy et de sécurité. Enfin, ces jugements correspondent à des annotations de vidéos YouTube de qualité, des objets d’étude autrement plus intéressants que des images MNIST ou CIFAR10. En particulier, on peut ainsi espérer comprendre et généraliser la notion de “qualité d’une vidéo” selon des jugements humains — une généralisation qui semble beaucoup plus désirable que la généralisation des comportements des utilisateurs des réseaux sociaux.

Mais pour arriver à être entendu de la communauté scientifique, pour détourner leur attention de la désinformation scientifique de Google et pour orienter leur recherche vers les problèmes d’éthique et de sécurité de l’information qui requièrent tant leur expertise, leur attention et leur travail, il va falloir crédibiliser Tournesol et valoriser sa base de données publiques.

Et ça, ça dépend énormément de vous. Vous pouvez grandement aider à améliorer la qualité de la recherche scientifique en intelligence artificielle, tout simplement en créant un compte sur Tournesol, et en comparant la recommandabilité de vidéos YouTube que vous avez visionnées sur la plateforme. Vous pouvez aussi contribuer à notre base de données en vous portant garants pour vos amis, et en leur demandant de se poser garants pour vous — ce qui, au passage, augmentera aussi vos droits de vote. Enfin, vous pouvez relayer les appels à contribuer à Tournesol, et par exemple demander à vos vulgarisateurs, journalistes et influenceurs préférés de parler davantage de ce projet, ou de partager les liens Tournesol plutôt que les liens YouTube, lorsque leurs vidéos ont été beaucoup notées.

La recherche en éthique et en sécurité est aujourd’hui hackée, dénigrée et ignorée. Elle a besoin de vous pour s’épanouir, et enfin contribuer à protéger les sociétés humaines contre les fléaux de la désinformation, du harcèlement et de la haine.

