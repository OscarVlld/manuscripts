# Zuckerberg doit-il finir en prison ?

En septembre 2021, le Wall Street Journal lâche une bombe. Il publie une série d'articles au sujet des #FacebookFiles, le plus grand leak de l'histoire des entreprises de l'information, révélé d'abord en exclusivité par la lanceuse d'alerte Frances Haugen, employée de l'équipe d'intégrité de Facebook. Dans ces #FacebookFiles, Frances Haugen a compilé de nombreux messages communiqués en interne par l'entreprise, qui révèlent de façon très compromettante de nombreuses décisions illégales et très immorales faites par les dirigeants de l'entreprise.  
https://www.wsj.com/articles/the-facebook-files-11631713039 

Voilà qui a conduit l'ancien investisseur de longue date de Facebook, Roger McNamee, à appeler à des poursuites judiciaires envers Facebook en général, et envers ses dirigeants en particulier, lors du Web Summit, pourtant habituellement dédié à des discussions techniques.

20:37 "Je pense qu'il y a au moins six domaines où des enquêtes criminelles sont justifiées. Les documents de Frances Haugen en montrent trois en particulier. 
Pour la Commission Sécurité et Échange, Facebook n'a clairement pas réussi à divulguer correctement aux investisseurs ce qu'il savait sur son activité. Dans certains cas, cela est passible d'une peine de prison. 
Deuxièmement, Facebook est clairement coupable d'avoir permis à de la traite d'humains d'avoir lieu sur sa plateforme, en toute connaissance de cause et en étant payé pour que cela se produise. Il s'agit également d'un crime, également passible de peines de prison pour les dirigeants. 
Les documents de Frances Haugen montrent également à quel point la direction de Facebook était complice de "Stop the Steal", qui a conduit aux insurrections. La complicité d'une insurrection est un crime passible d'une peine de prison. 
Dans une affaire antitrust, Google est poursuivi par le procureur général de l'État du Texas pour entente sur les prix avec Facebook dans le domaine de la publicité numérique. La peine standard pour cette affaire est de trois ans et demi de prison pour tous les dirigeants. Il s'agit du cas le plus clair de fixation des prix aux États-Unis depuis des décennies. 
Il y a d'autres cas. Ce que j'essaie de faire comprendre ici, c'est que pour que la loi ait un sens, nous devons la faire appliquer."  
https://www.youtube.com/watch?v=5k6cye15PM4

Même son de cloche du côté de John Tye, l'avocat de Frances Haugen.

10:45 "Quelles sont les lois qui, selon vous, ont été violées ?  
JT : En tant que société cotée en bourse, Facebook est tenu de ne pas mentir à ses investisseurs, ni même de dissimuler des informations. La Commission Sécurité et Échange engage donc régulièrement des actions coercitives, en alléguant que des sociétés comme Facebook et d'autres, font des omissions d'informations matérielles qui affectent les investisseurs de manière adversariale."  
https://www.youtube.com/watch?v=_Lx5VmAdZSI 

Tout ça pour dire que Zuckerberg en prison, ce n'est pas un fantasme de hater. Ce serait en fait avant tout une simple application de la loi qui existe déjà aujourd'hui - oui parce qu'on peut sérieusement se poser la question de l'utilité de régulations nouvelles, si celles d'aujourd'hui ne sont même pas appliquées. 

Il semble ainsi assez clair que Zuckerberg et ses collègues ont pris des décisions illégales. Mais aussi et surtout, la direction de Facebook a pris des décisions éthiquement désastreuses, qui ont conduit à au moins des dizaines de milliers de morts et des centaines de millions de traumatismes psychologiques à travers le monde, y compris dans les pays développés. Et selon les #FacebookFiles révélés par Frances Haugen, Zuckerberg et ses collègues le savaient pertinemment.

Aujourd'hui, je vais insister sur trois décisions particulièrement dérangeantes des dirigeants de Facebook, même s'il y aurait pourtant beaucoup plus encore à dire. Et je conclurai en affirmant publiquement mon soutien à tous les militants, journalistes, chercheurs et avocats, qui feront le travail de poursuite judiciaire de l'un des hommes les plus puissants et les plus irresponsables au monde. Plus que jamais, nous avons désespérément besoin d'eux pour protéger des milliards d'humains sur terre de la désinformation et des appels à la haine et à la violence, et des nombreuses conséquences tragiques qui en découlent.

## Complicité de trafic d'humain

0:46 Quand elle avait 16 ans, un homme plus âgé lui a envoyé une demande d'ami. 
"Quand j'étais plus jeune, il s'agissait de savoir qui avait le plus d'amis. Alors si quelqu'un vous envoie une demande, ajoutez-le."  
Il lui a dit qu'elle pouvait gagner 2 000 $ par semaine en étant mannequin. Après une dispute avec sa mère, elle l'a rencontré.  
"Il a pris mon téléphone pour que je ne puisse pas m'enfuir. Je ne savais pas où j'étais."
Dans cet hôtel, l'homme a pris des photos suggestives et les a mises en ligne. En quelques heures, des hommes payaient pour la violer.  
https://www.youtube.com/watch?v=u5Sv-qq5oJI 

Le crime le plus illégal que Marc Zuckerberg a probablement commis est sans doute la complicité de trafic d'humains, notamment de femmes et de mineurs à des fins sexuelles. C'est en tout cas l'accusation que l'avocate Annie McAdams est en train de constituer pour poursuivre Facebook en justice.  
https://www.nytimes.com/2019/12/03/technology/facebook-lawsuit-section-230.html 

1:26 "Vous avez déposé le dossier il y a trois ans. Que s'est-il passé depuis ?  
"Des délais après délais, appels après appels."  
Facebook affirme que l'affaire devrait être rejetée, que l'entreprise est protégée par la section 230 de la loi de 1996 sur la décence des communications, interprétée depuis longtemps par les tribunaux comme le fait que les sites de médias sociaux ne sont pas légalement responsables du contenu que les utilisateurs publient. Mais McAdams affirme que Facebook et ses algorithmes font partie du problème.  
"Ce que nous constatons ici, ce sont les actions indépendantes que Facebook entreprend pour mettre en relation ces prédateurs et ces enfants. Et comment ils tirent profit des connexions qui en résultent".  
https://www.youtube.com/watch?v=u5Sv-qq5oJI 

Et alors, pendant longtemps, le dossier d'Annie McAdams manquait d'éléments tangibles pour montrer que Zuckerberg et les autres dirigeants de Facebook sont tout à fait conscients de l'ampleur du problème, et prennent des mesures très largement insuffisantes pour le régler. Sauf que les #FacebookFiles révélés par Frances Haugen vont très largement dans le sens d'Annie McAdams.

1:32 "Justin Scheck : Des gens ont été achetés et vendus sur Facebook.  
Kate Linebaugh : Est-ce que Facebook le sait ?  
Justin Scheck : Facebook est au courant depuis au moins quelques années et n'a pas pris de mesures efficaces pour y mettre fin."  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-3-this-shouldnt-happen-on-facebook/0ec75bcc-5290-4ca5-8b7c-84bdce7eb11f

4:54 "Kate Linebaugh : Les trafiquants ont créé plus d'une centaine de faux comptes Facebook et Instagram pour recruter des femmes en leur promettant un travail bien rémunéré.  
Justin Scheck : Ils utilisaient ces comptes Facebook et Instagram pour dire, regardez, nous avons des emplois bien payés aux Émirats arabes unis. Venez aux Émirats arabes unis. Les trafiquants utilisaient Facebook Messenger et WhatsApp, qui appartient également à Facebook, pour coordonner le voyage des femmes. Ce qui était incroyable, c'est que, comme ils avaient accès aux messages Facebook et Messenger, ils ont pu voir les communications entre les femmes et leurs employeurs, ainsi qu'entre les femmes et leurs amis et contacts.  
Kate Linebaugh : Dans ces messages, l'équipe de Facebook pouvait voir ce qui arrivait aux femmes. Une fois arrivées à destination, aucun emploi bien rémunéré ne les attendait. Au lieu de cela, les femmes se sont vu retirer leur passeport et ont été forcées à se prostituer."  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-3-this-shouldnt-happen-on-facebook/0ec75bcc-5290-4ca5-8b7c-84bdce7eb11f

Et bien sûr, il ne s'agit ici que d'un cas parmi tant d'autres de trafic d'humains, avec des effets au moins comparables à de l'esclavage et du viol. D'après l'organisation mondiale du travail, de tels traffics font des dizaines de millions de victimes à travers le monde.  
https://www.ilo.org/global/about-the-ilo/newsroom/news/WCMS_574719/lang--fr/index.htm

OK, il y a donc clairement des choses horribles qui ont lieu sur Facebook. Mais Facebook est-il au courant ? Et surtout Facebook prend-il des mesures à la hauteur de l'atrocité de ces trafics ?

13:41 Justin Scheck : [Des employés] ont trouvé du contenu sur les sites de Facebook qui violait la politique de servitude domestique de l'entreprise et qui aurait dû être automatiquement signalé par cet outil de détection. Mais le contenu n'a pas été signalé parce que l’outil de détection a expiré. Il n'était plus actif. Et nous pouvons voir dans les documents que cet employé demande à l'entreprise de réactiver l’outil de détection, mais il reste inactif.  
Kate Linebaugh : Interrogé à ce sujet, Facebook a déclaré avoir des dizaines d'outils logiciels similaires actuellement en fonctionnement qui aident à identifier les contenus liés à la traite des êtres humains. Un autre document décrit un plan de campagne visant à cibler les messages sur Facebook pour les victimes potentielles de la traite des êtres humains, en les informant de leurs droits.  
Justin Scheck : Et ce document de planification suggère à l'entreprise d'utiliser du tacte dans des avertissements en langue arabe sur la traite d'humains afin de ne pas "aliéner" les acheteurs, c'est-à-dire les utilisateurs de Facebook qui achètent la main-d'œuvre domestique sous forme de contrats souvent dans des situations proches de l'esclavage.  
Kate Linebaugh : Aliéner les acheteurs ?  
Justin Scheck : Oui. Facebook est très préoccupé par le fait que toute mesure de protection qu'il prend pourrait avoir pour effet second involontaire de réduire le nombre de personnes sur Facebook. C'est une préoccupation omniprésente. Et par conséquent, ils finissent souvent par ne pas prendre de mesures jusqu'à ce qu'il y ait une crise.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-3-this-shouldnt-happen-on-facebook/0ec75bcc-5290-4ca5-8b7c-84bdce7eb11f

Autrement dit, dans ces messages, Facebook prend littéralement la défense des acheteurs d'humains ; ces sortes de "propriétaires" d'esclaves. Tout ça parce que ces acheteurs sont une source importante de profits pour Facebook, car ce sont les cibles de nombreuses publicités.

16:22 Justin Scheck : La BBC avait travaillé sur ce documentaire qui montrait que les femmes au Koweït étaient achetées et vendues comme domestiques. En gros, du travail forcé. Avant que le documentaire ne soit diffusé, la BBC a contacté Facebook et a dit : "Nous avons trouvé toutes sortes de violations." [...]  
Kate Linebaugh : Facebook a fait ce qu'il avait fait auparavant. Il a retiré le contenu offensant.
Justin Scheck : Donc, ils ont essentiellement supprimé le mauvais comportement, mais ils n'ont pas changé quoi que ce soit de systémique sur ce qui, dans Facebook, permettait à ce genre de choses de se produire.  
Kate Linebaugh : Initialement, cela semblait être la fin de l'affaire.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-3-this-shouldnt-happen-on-facebook/0ec75bcc-5290-4ca5-8b7c-84bdce7eb11f

La pression journalistique n'aura eu quasiment aucune conséquence, si ce n'est la suppression d'une poignée des innombrables activités de trafic d'humains. Facebook n'a fait que le minimum pour empêcher la publication de l'article de la BBC. Mais Facebook n'a pas investi les ressources et moyens dont ils disposent pour combattre sérieusement le problème.

Pour protéger des millions de victimes de ce trafic d'humains, la BBC a toutefois eu la brillante idée d'informer Apple de la complicité de Facebook dans le trafic d'humains.   
https://www.bbc.com/news/technology-58645547 

17:14 Apple était furieux que le trafic d'humains se déroule sur Facebook et a dit à Facebook que s'il ne résolvait pas ce problème sur ses plateformes, Apple retirerait les applications Facebook et Instagram de son App Store. Et nous le savons grâce à un document interne de Facebook appelé Apple Escalation on Domestic Servitude. Et ce document est une sorte de post-mortem, montrant comment Facebook a traité la demande d'Apple de supprimer le trafic humain. Alors que le message de la BBC disant que des gens sont achetés et vendus sur Facebook n'a pas été traité comme une urgence au sein de Facebook, le message d'Apple a été traité comme une urgence.  
[...]  
Kate Linebaugh : Confronté à un grand risque pour l'entreprise, Facebook a pris plus de mesures pour arrêter le trafic d'êtres humains qu'après des enquêtes comme celle d'Op Dubaï. Et plus qu'après que la BBC les ait contactés. Dans le document d'escalade d'Apple, les auteurs disent :
Justin Scheck : "Pour atténuer ce risque. Nous avons fait partie d'un grand groupe de travail fonctionnant 24 heures sur 24 pour développer et mettre en œuvre notre stratégie de réponse."  
[...]  
Justin Scheck : Ils ont été en mesure d'identifier plus de 300 000 posts qu'ils ont qualifié de “violant potentiellement la politique de Facebook”, de désactiver, de supprimer des milliers de comptes, dans certains cas de désactiver des appareils, ce qui signifie empêcher certains appareils d'avoir des comptes avec la société.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-3-this-shouldnt-happen-on-facebook/0ec75bcc-5290-4ca5-8b7c-84bdce7eb11f 

Comme quoi, quand Facebook le veut, ils sont capables de faire des efforts pour vraiment lutter contre le trafic d'humains. Mais clairement, ce n'est que lorsque leur business est menacé que Facebook va faire de tels efforts
Voilà qui montre à quel point les discours de Facebook sur l'importance assignée à la sécurité de ses utilisateurs est, en temps normal, du pur bullshit. 

20:15 Kate Linebaugh : Ces mesures ont été suffisantes pour faire reculer Apple. Les applications de Facebook sont restées dans l'App Store. [...] Mais malgré cela, la traite des êtres humains a continué sur la plateforme.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-3-this-shouldnt-happen-on-facebook/0ec75bcc-5290-4ca5-8b7c-84bdce7eb11f 

La stratégie de la BBC n'aura donc été que partiellement productive. Si l'on veut vraiment combattre efficacement l'esclavagisme sexuel à travers le monde, il semble urgent de disposer de moyens de pression beaucoup plus efficaces. À ce titre, poursuivre les dirigeants de Facebook en justice, avec des peines de prison potentielles qui me semblent tout à fait justifiables et justifiées, ce ne serait certainement pas inutile... 

## Connivence avec des dictateurs

05:50 Jeff Horwitz : "Pour quelques membres choisis de notre communauté. Nous n'appliquons pas nos politiques et nos normes. Contrairement au reste de notre communauté, ces personnes peuvent violer nos normes sans aucune conséquence."  
Ryan Knutson : Wow. Donc, chez Facebook, il y a une liste blanche de personnes qui peuvent dire ce qu'elles veulent sur la plateforme, violer les règles et ne pas être automatiquement virées de Facebook ou voir leurs publications supprimées.  
Jeff Horwitz : Oui. La première fois que j'ai vu le document, je me suis écrié whoa".  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-1-the-whitelist/aa216713-15af-474e-9fd4-5070ccaa774c 

Comme toutes les plateformes du web, y compris celles qui prétendent n'avoir aucune règle, Facebook a des Conditions Générales d'Utilisation. En fait, de par la loi, Facebook est tenu de supprimer de sa plateforme les contenus illégaux ; et Facebook déclare vouloir retirer la pornographie, le cyber-harcèlement et les appels au meurtre, de sa plateforme.  
https://www.youtube.com/watch?v=c5JfOHyfFP0  
https://transparency.fb.com/fr-fr/policies/community-standards/ 

Sauf qu'un beau jour, Facebook a eu le malheur d'appliquer ses propres règles à une célébrité.

7:40 "la chanteuse Rihanna s'est fait fermer temporairement son compte sur Instagram, qui appartient à Facebook, après avoir posté une photo d'elle partiellement nue sur la couverture d'un magazine français. L'erreur a fait la une des journaux et Facebook a dû faire publiquement marche arrière.
Jeff Horwitz : Il leur fallait donc trouver un moyen d'empêcher les personnes importantes de se faire expulser sommairement de la plateforme ou de voir leur contenu supprimé.  
Ryan Knutson : La réponse de Facebook à ce problème était un système appelé Shielding en interne.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-1-the-whitelist/aa216713-15af-474e-9fd4-5070ccaa774c 

En fait, on a là une formalisation institutionnelle d'une inégalité flagrante entre les utilisateurs des réseaux sociaux. Les influenceurs, ceux qui sont suivis par un grand public, ont un pouvoir disproportionné. Tout le monde peut se plaindre sur les réseaux sociaux. Mais quand la personne qui se plaint le fait auprès de ses 100 million d'abonnés, les coûts pour la personne ou l'entreprise critiquée sont de l'ordre d’un million de fois plus importants que s'il s'agit d'un utilisateur lambda.

Et ça, dans le cas de Facebook, ça a conduit petit à petit à des passe-droits, initialement appelés Shielding, et plus tard CrossCheck, dont bénéficient les gens influents. Ainsi, Facebook est littéralement en train de donner beaucoup plus de pouvoir à ceux qui en ont probablement déjà beaucoup trop. Comme Neymar, le joueur de football brésilien du Paris Saint-Germain.

12:52 Ryan Knutson : "En 2019, une femme a accusé Neymar de viol. Neymar [...] a nié les allégations et n'a jamais été inculpé. Dans le cadre de son démenti, il a diffusé en direct une vidéo de lui-même sur Instagram et Facebook [...] Il a parcouru les messages que la femme lui avait envoyés, révélant son nom et des photos dénudées ".  
Jeff Horwitz : C'est un interdit sur la plateforme de Facebook, c'est ce qu'on appelle la nudité non consensuelle, alias le porno de vengeance, c'est complètement interdit et les politiques de Facebook à ce sujet sont très claires, c'est-à-dire que selon leurs directives opérationnelles, leurs documents internes, la façon dont vous gérez cela est de retirer immédiatement le contenu, évidemment. Ensuite, vous supprimez définitivement le compte qui l'a publié, n'est-ce pas ? C'est l'idée de la tolérance zéro. Vous ne pouvez pas faire ça à quelqu'un. Sauf que Neymar fait partie de Crosscheck.  
Ryan Knutson : Parce que Neymar était dans le système Crosscheck. Facebook n'a pas pris les mesures habituelles, à savoir supprimer la publication incriminée, puis supprimer son compte
[...]
Le document indique qu'un employé de Facebook a essayé de supprimer la publication de Neymar le samedi où elle a été publiée, mais le système de Facebook l'a empêché de le faire.  
Jeff Horwitz : Et pendant les 24 heures suivantes, cette vidéo dans laquelle Neymar a essentiellement montré au monde le nom de cette femme et des photos d'elle nue est restée en ligne et a été vue bien au-delà de 50 millions de fois.  
Ryan Knutson : Wow.  
Jeff Horwitz : Ce qui s'est passé immédiatement après, c'est que tout d'abord, la femme a été harcelée, incroyablement sur la plateforme.  
Ryan Knutson : La femme a été inondée de harcèlement et d'intimidation en ligne, Facebook a supprimé plus de 3 500 comptes de personnes se faisant passer pour elle, mais Neymar n'a pas subi beaucoup de conséquences pour avoir enfreint les règles de Facebook.  
Jeff Horwitz : Neymar n'est pas seulement célèbre. Il est très célèbre et virer de sa plateforme des stars des réseaux sociaux très célèbres et très photogéniques n'est pas vraiment dans l'intérêt de Facebook. Ce qui s'est passé, c'est qu'après avoir consulté les hauts responsables de l'entreprise, et ils ne précisent pas qui, ils ont décidé qu'ils allaient simplement supprimer la publication et non pas supprimer le compte de Neymar.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-1-the-whitelist/aa216713-15af-474e-9fd4-5070ccaa774c 

Et alors, je suis sûr que parmi vous, il y a des fans de football, beaucoup de fans du PSG, et même beaucoup de fans de Neymar en particulier. Et vous pourriez vouloir défendre Neymar.

Mais en fait, le cas de Neymar, pourtant horrible à mon goût, n'est en fait qu'une goutte d'eau dans un océan de contenus ignobles mis en ligne avec impunité par des gens surpuissants aux discours très nocifs. Oui, parce que dans la whitelist de Facebook, il y a toutes sortes de gens très puissants et très suivis. Et cette liste inclut en particulier aussi des dictateurs.

Ryan Knutson : Mais qu'en est-il des comptes de personnes très puissantes qui sont également connues pour diffuser des fausses informations comme les dictateurs autoritaires, ou des personnes avec un grand nombre de followers qui ne font que diffuser des théories du complot ? Ont-ils été inclus dans cette liste ?  
Jeff Horwitz : Oui. Si vous dépassiez une certaine taille, vous étiez protégé.  
Ryan Knutson : À cause de cette protection, les utilisateurs puissants pouvaient poster des choses comme des discours de haine ou des incitations à la violence et cela restait affiché.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-1-the-whitelist/aa216713-15af-474e-9fd4-5070ccaa774c 

Et dans certains cas, et malheureusement j'ai envie de dire dans de plus en plus de cas, ce whitelisting de dictateurs autoritaires a conduit à des conflits armés, voire à des génocides. D'ailleurs, dès 2018, l'Organisation des Nations Unis a souligné le rôle de Facebook dans les génocides des Rohingyas au Myanmar, qui a conduit à des dizaines de milliers de meurtres et de viols, et au plus grand camp de réfugiés de l'histoire à Kutupalong au Bangladesh, où vivent près de 600 000 victimes de guerre et de racisme.  
https://www.ohchr.org/EN/HRBodies/HRC/Pages/NewsDetail.aspx?NewsID=23575&LangID=E  
https://about.fb.com/news/2018/11/myanmar-hria/  
https://fr.wikipedia.org/wiki/G%C3%A9nocide_des_Rohingya 

Sachant que, au Myanmar, Facebook est l'application par défaut dans tous les téléphones, à travers le projet Internet.org, renommé depuis en Free Basics, et finalement interrompu au Myanmar suite au torrent de désinformations et d'appels à la haine communiqué via Facebook dans ces pays, Facebook semble clairement extrêmement complice dans cette tragédie monumentale, d’autant que, en 2014, l’entreprise n’avait qu’un seul modérateur parlant birman pour 7 millions d’utilisateurs. En tout cas, c'est ce qu'enquête désormais un juge fédéral américain.  
https://www.lemonde.fr/international/article/2021/09/23/exactions-contre-les-rohingya-en-birmanie-facebook-dans-le-viseur-d-un-juge-americain_6095758_3210.html  
https://techcrunch.com/2018/05/01/facebook-free-basics-ending-myanmar-internet-org  
https://www.lemonde.fr/pixels/article/2018/08/16/en-birmanie-l-echec-de-facebook-contre-l-incitation-a-la-haine-et-les-fausses-informations_5343078_4408996.html  
https://www.letemps.ch/monde/facebook-pointe-doigt-role-crise-rohingyas

Et bien sûr, je ne parle ici que du cas du Myanmar, car il a été relativement bien documenté, mais comme le signale la lanceuse d'alerte Sophie Zhang, des catastrophes similaires ont lieu aux quatre coins du monde, de l'Inde à l'Éthiopie, en passant par le Honduras, l'Azerbaïdjan, le Brésil, les États-Unis, et même la France.  
https://restofworld.org/2021/facebook-papers-reveal-staggering-failures-in-global-south/  
https://www.lemonde.fr/pixels/article/2021/04/24/manipulations-politiques-en-ligne-la-lanceuse-d-alerte-sophie-zhang-denonce-les-lacunes-de-facebook_6077877_4408996.html 

"Malheureusement, Facebook s'est rendu complice par son inaction de cette répression autoritaire", a-t-elle écrit. "Bien que nous ayons lié de manière concluante ce réseau à des éléments du gouvernement au début du mois de février [2020], et que nous ayons compilé de nombreuses preuves de sa nature violatrice, la décision effective a été prise de ne pas en faire une priorité, fermant effectivement les yeux."  
https://www.theguardian.com/technology/2021/apr/12/facebook-fake-engagement-whistleblower-sophie-zhang?CMP=series_embed_box#:~:text=%E2%80%9CMalheureusement%2C%20Facebook%20has,a%20blind%20eye.%E2%80%9D 

16:13 Ryan Knutson : Des personnes à l'intérieur de Facebook ont en fait essayé de déterminer le nombre de vues de ce genre de mauvaises publications, des publications qui auraient autrement été retirées si les comptes n'étaient pas très connus et protégés par Crosscheck.  
Jeff Horwitz : Et le chiffre qu'ils ont trouvé pour 2020 était de plus de 16 milliards.  
Ryan Knutson : 16 milliards de vues ?  
Jeff Horwitz : Vues.  
Ryan Knutson : Du contenu qui aurait dû être retiré ?  
Jeff Horwitz : Oui, le contenu qui, selon la détermination finale de Facebook après de multiples couches de révision, violait définitivement ses règles, a été vu 16,4 milliards de fois.  
Ryan Knutson : C'est 16,4 milliards de vues de choses comme les discours de haine, le racisme, le porno de vengeance, la violence graphique. Ce chiffre est si élevé parce que, rappelons-le, il s'agit de comptes très en vue avec un nombre considérable de followers. Leurs posts atteignent instantanément un grand nombre de personnes.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-1-the-whitelist/aa216713-15af-474e-9fd4-5070ccaa774c 

## Marchand de haine

4:46 Haugen nous a dit que la racine du problème de Facebook est un changement qui a été fait en 2018 à ses algorithmes, le programme qui décide ce que vous voyez sur votre fil d'actualité Facebook.
'Vous avez votre téléphone. Vous pouvez ne voir qu'une centaine de contenus, si vous vous asseyez et faites défiler. Mais Facebook a des milliers d'options qu'il peut vous montrer".
L'algorithme choisit parmi ces options en fonction du type de contenu avec lequel vous vous êtes le plus engagé dans le passé.
Et l'une des conséquences de la façon dont Facebook sélectionne ce contenu aujourd'hui est qu'il optimise le contenu qui obtient des engagements ou des réactions. Mais ses propres recherches montrent que les contenus haineux, diviseurs et polarisants inspirent plus facilement la colère que d'autres émotions".
La désinformation et les contenus pleins de colère attirent les gens et les gardent sur la plateforme. 
'Oui, Facebook a réalisé que s'il change l'algorithme pour être plus sûr, les gens passeront moins de temps sur le site, ils cliqueront sur moins de publicités, ils gagneront moins d'argent'.  
https://www.youtube.com/watch?v=_Lx5VmAdZSI 

On en arrive à ce qui est je pense, le pire aspect de tout Facebook : l'algorithme.
Et là, malheureusement, la loi n'est pas tout à fait adaptée pour juger un algorithme aussi puissant et influent que celui de Facebook, ou plutôt pour juger ceux qui ont conçu, approuvé et déployé un tel algorithme. Et ça, malheureusement, ça rend l'incrimination de Facebook et Zuckerberg à cause du choix de l'algorithme délicate.
Néanmoins, je vais affirmer que cet algorithme peut être largement vu comme étant un trafiquant de haine, qui révèle plus largement le fond de commerce de Facebook.
Plus précisément, Facebook est un intermédiaire qui, de plus en plus, finance et encourage les producteurs de haine à être toujours plus dans l'extrême, et qui, pour faire du profit, diffuse la haine à des milliards d'utilisateur, quitte à faire subir des violences effroyables à des centaines de millions d'entre eux.  
https://www.technologyreview.com/2021/11/20/1039076/facebook-google-disinformation-clickbait/ 

De fait, Facebook est devenu un marchand de haine. Mais surtout, la direction de Facebook a pris des mesures scandaleuses pour conforter ce modèle d'affaire aux externalités catastrophiques. D'un point de vue moral, tel est, je pense, le plus grand des crimes de Facebook. Et oui, notamment d'un point de vue conséquentialiste, à travers le monde, nous sommes déjà en train d'assister aux conséquences désastreuses de ce modèle d'affaire. Car plus de haine, c'est plus de tensions, qui peuvent aller jusqu'à causer des violences armées, notamment aux États-Unis, en Inde et en Éthiopie, et peut-être bientôt entre le Maroc et l'Algérie.  
https://www.lemonde.fr/afrique/article/2021/11/25/l-algerie-en-quete-d-une-riposte-a-l-axe-militaire-maroc-israel_6103620_3212.html 

7:36 Facebook amplifie essentiellement le pire de la nature humaine. 
C'est l'une de ces conséquences malheureuses. Personne chez Facebook n'est malveillant, mais les incitations sont mal alignées. Facebook gagne plus d'argent lorsque vous consommez plus de contenu. Les gens aiment s'engager avec des choses qui suscitent des réactions émotionnelles, et plus ils sont exposés à la colère, plus ils interagissent, plus ils consomment. 
Cette dynamique a conduit à une plainte de Facebook par les principaux partis politiques à travers l'Europe. Ce rapport interne de 2019 obtenu par Haugen indique que les partis politiques sont convaincus que la modification de l'algorithme les a forcés à adopter une attitude négative dans leurs communications sur Facebook, ce qui les a conduits à des positions politiques plus extrêmes. Les partis politiques européens disaient essentiellement à Facebook : la façon dont vous avez écrit votre algorithme change la façon dont nous dirigeons nos pays. Vous nous obligez à prendre des positions que nous ne voulons pas, que nous savons mauvaises pour les sociétés. Nous savons que si nous ne prenons pas ces positions, nous ne gagnerons pas sur le marché des médias sociaux.
https://www.youtube.com/watch?v=_Lx5VmAdZSI 

Mais, me demanderez-vous, est-on vraiment sûr que l'algorithme de Facebook joue vraiment un rôle dans l'amplification de la haine ? N'y a-t-il pas des articles scientifiques publiés qui suggèrent que, au contraire, le rôle des algorithmes est exagéré ?

Alors, bien sûr, je n'ai pas lu toute la littérature scientifique à ce sujet. Mais le peu que j'ai lu n’est en fait pas de très grande qualité, avec des conclusions survendues à partir de sondages purement déclaratifs sur le sentiment de polarisation des sondés, mesurés sur des effets à beaucoup trop court terme, ou sujets au paradoxe de Simpson. Certains titres me semblaient même exprimer un grave excès de confiance, sachant la complexité du problème et le manque de données fiables pour l’analyser.  
https://wiki.tournesol.app/wiki/Radicalization  
https://www.tandfonline.com/doi/full/10.1080/1369118X.2018.1444076  
https://snurb.info/files/2019/It%e2%80%99s%20Not%20the%20Technology,%20Stupid.pdf 

En fait, il faut bien se rendre compte que les académiques n'ont aucun moyen de conduire des expériences fiables à grande échelle de l'effet d'exposition répétée à grande échelle à des discours de haine, sur des intervalles de temps de plusieurs années. Au contraire, les données auxquelles ces académiques ont accès sont désespérément pauvres et incomplètes, notamment à cause de l'opacité abusive de Facebook.  
https://www.youtube.com/watch?v=gQHvTow91FY&list=PLgqL_7nXb23FKk_rUfs7vnvyrPshYPfA8&index=3  
https://www.youtube.com/watch?v=_RuyXyekx6g&list=PLgqL_7nXb23FKk_rUfs7vnvyrPshYPfA8&index=4 

D'ailleurs, à ce sujet, et j'en parlerai plus une prochaine fois, Google me semble s'être engagé dans une très dangereuse désinformation scientifique, avec la publication d'articles aux titres très mensongers, et des tentatives d'interdiction ou de modifications en profondeur d'articles critiques des produits de Google... Ça ne m'étonnerait absolument pas que Facebook en fasse de même, même si de ce que je vois, tout ça m'a l'air bien pire encore chez Google.  
https://arxiv.org/abs/2108.10241  
https://arxiv.org/abs/2110.05679  
https://dl.acm.org/doi/10.1145/3442188.3445922  
https://arxiv.org/abs/2012.07805  
https://www.reuters.com/article/us-alphabet-google-research-focus-idUSKBN28X1CB 

À l'inverse, les #FacebookFiles, eux, rapportent ce qui ressemble le plus à une expérience scientifique, ou du moins à une expérience naturelle, pour tester l'impact des algorithmes. Et pour cela, il nous faut revenir en 2017.

3:47 "2017 a été ce qu'un employé de Facebook a appelé une annus horribilis.
Ryan Knutson : Annus horribilis, c'est du latin pour une année horrible. La raison pour laquelle l'employé a qualifié 2017 d'année horrible est que Facebook connaissait un changement affligeant dans le comportement des utilisateurs.  
Keach Hagey : Les mesures d'engagement ont décliné de manière alarmante tout au long de 2017, de différentes manières."  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-4-the-outrage-algorithm/e619fbb7-43b0-485b-877f-18a98ffa773f 

Moins d'engagements, c'est moins de clics. Et moins de clics, c'est moins d'argent pour Facebook. Les dirigeants de Facebook se devaient de réagir.

6:50  Ryan Knutson : Une fois que les scientifiques de Facebook ont identifié ce problème d'engagement, la société a mis au point un plan. Pour inciter les utilisateurs à publier, commenter et interagir davantage pendant le temps qu'ils passent sur Facebook, l'entreprise allait apporter un changement majeur à ce que les utilisateurs voyaient dans leur fil d'actualité. Ils allaient remanier l'algorithme.  
Intervenant 6 : Au fil du temps, nous pensons que les gens verront plus de publications de personnes avec lesquelles ils sont connectés et moins de contenu provenant d'éditeurs.  
Ryan Knutson : Dans une vidéo de Facebook annonçant le changement, la société a déclaré que le pivot était destiné à rendre la plateforme plus saine et à améliorer le bien-être de ses utilisateurs.  
Intervenant 6 : Et au cours de l'année prochaine, la mission de notre équipe est de vous aider à avoir des interactions plus profondes et plus significatives avec les personnes qui vous intéressent. Notre objectif est que le temps que les gens passent sur Facebook soit meilleur.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-4-the-outrage-algorithm/e619fbb7-43b0-485b-877f-18a98ffa773f 

Autrement dit, là, on n'en est même pas à faire de l'ethics washing. Facebook n'est pas en train de faire des déclarations et prendre des mesures pour répondre à un besoin éthique. Les dirigeants de Facebook sont en train là d'inventer un problème éthique, pour faire passer en douce un changement profond de l'algorithme, dont le vrai but est en fait clairement une maximisation de profits au détriment de l’éthique.

10:29 Ryan Knutson : Une diapositive d'une présentation interne explique que les publications originales, que Facebook considère comme la mesure ultime du succès, étaient en hausse après le changement de [l'algorithme], après des années de déclin continu.  
Keach Hagey : Mais du point de vue de ce pour quoi ils ont dit que c'était fait, à savoir augmenter le bien-être, c'était un échec.  
Ryan Knutson : Les recherches internes de Facebook, que The Journal a examinées, ont révélé que, bien que l'engagement ait augmenté, rien ne prouve que les utilisateurs se sentent mieux par rapport au temps qu'ils passent sur la plateforme.  
Keach Hagey : En fait, les gens aimaient un peu moins ce qu'il y avait dans leur fil d'actualité qu'avant le changement. Les gens étaient donc moins heureux et ces études ont persisté tout au long de 2018 et ont à peu près constamment montré la même chose.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-4-the-outrage-algorithm/e619fbb7-43b0-485b-877f-18a98ffa773f 

En fait, en termes éthiques, le nouvel algorithme était absolument catastrophique. Et ça, Facebook a pu le mesurer comme aucun scientifique en dehors de Facebook ne peut le faire, faute de données et de moyens d'expérimentation.

16:14 Keach Hagey : Encore et encore, dans les mois qui ont suivi, les scientifiques des données [de Facebook] continuent de concevoir des expériences disant : Hé, les gens pensent que ce changement d'algorithme rend Facebook plus diviseur et sensationnel. Les éditeurs de presse disent que cela les oblige à produire du mauvais contenu. C'est un problème. Nous devons faire quelque chose à ce sujet. Nous devons trouver comment modifier l'algorithme pour supprimer ces incitations à créer autant, comme une personne l'a appelé, d'appât à l'indignation."  
Ryan Knutson : Les documents montrent qu'après le changement d'algorithme, le contenu diviseur a augmenté sur Facebook. Des posts sensationnels, extrêmes, et des choses comme la désinformation devenaient plus virales sur la plateforme.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-4-the-outrage-algorithm/e619fbb7-43b0-485b-877f-18a98ffa773f 

En réponse aux critiques internes, Facebook a alors créé une nouvelle équipe au sein de Facebook, appelée "équipe d'intégrité", et connue en interne comme la "Civic Team". 

18:27 Ryan Knutson : L'équipe civique a découvert que le nouvel algorithme poussait en fait la désinformation et le contenu toxique. Et non seulement cela, mais selon les documents, plus un contenu était partagé de fois, plus il était susceptible d'être faux.  
Jeff Horwitz : Ça semble presque trop simple, mais littéralement, à chaque fois qu'une chose est repartagée, ça empire. Donc, si une chose a été partagée 20 fois de suite, il y a 10 fois plus de chances qu'elle contienne de la nudité, de la violence, des discours de haine, de la désinformation, qu'une chose qui n'a pas été partagée du tout.  
Ryan Knutson : Les documents montrent que les chercheurs en intégrité ont commencé à proposer des solutions qui pourraient potentiellement empêcher tant de contenu négatif de devenir viral. L'une de leurs idées était de réduire [la quête d'engagement] en aval, la partie de la nouvelle formule qui faisait des prédictions sur le contenu le plus susceptible de devenir viral et le montrait à davantage d'utilisateurs.  
Jeff Horwitz : En termes pratiques, cela signifie que Facebook pourrait vous recommander quelque chose qui deviendrait viral même s'ils pensaient que vous aimeriez davantage un autre contenu. C'était quelque chose que les gens de l'intégrité disaient : "C'est fou. Vous ne faites qu'empirer les choses. C'est instable".  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-4-the-outrage-algorithm/e619fbb7-43b0-485b-877f-18a98ffa773f 

En particulier, on a là de très loin l'expérience la plus concluante sur l'impact des algorithmes de recommandation, longtemps controversés au sein de la littérature scientifique, malgré plusieurs publications antérieures de Facebook et Google déjà très compromettantes.  
https://www.youtube.com/watch?v=Z2G4q-E_oHA&list=PLtzmb84AoqRTl0m1b82gVLcGU38miqdrC&index=29  
https://www.youtube.com/watch?v=_RuyXyekx6g&list=PLgqL_7nXb23FKk_rUfs7vnvyrPshYPfA8&index=4 

Et comme l'explique si bien Veritasium notamment, les algorithmes n'affectent pas uniquement le public. Ils modifient en profondeur les incentives des producteurs de contenus, et donc l'offre médiatique générale, promouvant ainsi le plus détestable, et noyant au passage les informations très importantes, qui devraient absolument être connues par un beaucoup plus grand public.  
https://www.youtube.com/watch?v=fHsa9DqmId8  
https://twitter.com/le_science4all/status/1460895505718910984 

14:41 Keach Hagey : Donc, en octobre 2018, Jonah Peretti, le PDG de Buzzfeed, a envoyé un courriel à un haut dirigeant de Facebook disant que [l'algorithme] était fondamentalement en train de se retourner contre eux. Que cette chose qui a été créée théoriquement pour encourager des interactions sociales significatives ne créait pas d'interactions sociales significatives et en fait incitait les éditeurs comme Buzzfeed et d'autres à faire le très mauvais type de contenu, le type de contenu le plus toxique. Et il a fourni un exemple de Buzzfeed pour illustrer ce dont il parlait. Il a dit : " Écoutez, nous faisons toutes ces choses formidables sur les animaux et les soins personnels qui sont de haute qualité et devraient devenir virales, mais ce n'est pas le cas. Au lieu de cela, ce qui devient viral, ce sont des posts comme ce post que nous avons fait intitulé 21 choses que presque tous les Blancs sont coupables de dire, qui a récolté 13 000 partages et 16 000 commentaires sur Facebook." Et c'est devenu complètement viral. La plupart des gens s'engueulaient dans les commentaires à propos de la race et en voulaient à Buzzfeed pour l'avoir écrit en premier lieu. Et c'est cette frénésie de commentaires furieux qui a propulsé la viralité.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-4-the-outrage-algorithm/e619fbb7-43b0-485b-877f-18a98ffa773f 

Clairement, dans cette affaire Facebook n'est absolument pas qu'un simple intermédiaire inerte, et encore moins neutre. D'ailleurs, les #FacebookFiles révèlent que Facebook a activement choisi de booster les posts avec des emojis "colère". Ainsi, à chaque fois que vous exprimez votre énervement vis-à-vis d'un message Facebook, au lieu de réduire la propagation de ce message comme vous le souhaiteriez peut-être, Facebook l'amplifie pour énerver bien plus d'utilisateurs. En fait, Facebook a fait en sorte que chaque emoji colère compte autant que 5 likes !  
https://www.washingtonpost.com/technology/2021/10/26/facebook-angry-emoji-algorithm/ 

Voilà pourquoi je pense qu'on peut très largement affirmer que Facebook a endossé quasi-explicitement le rôle de trafiquant de haine à l'échelle planétaire. Ils ne sont clairement pas les seuls - Google et YouTube me semblent faire pire. Mais ils n'ont absolument rien d'innocents.

En tout cas, ceci a fortement déplu à sa Civic Team, qui voulait lutter contre la désinformation, le harcèlement et la haine. Cette équipe a proposé de nombreuses solutions aux dirigeants de l'entreprise pour rendre Facebook plus sécurisé et accueillant. Comment ces dirigeants ont-ils réagi à ces propositions ?

21:49 Jeff Horwitz : Donc un data scientist senior est allé voir les dirigeants de Newsfeed et a fait une présentation du genre : "Hey, idée brillante pour changer ce produit et résoudre tous nos problèmes d'intégrité. Tuer le bouton de partage, juste le tuer."  
Ryan Knutson : Supprimez le bouton "reshare".  
Jeff Horwitz : Supprimez juste les partages, c'est fait.  
Ryan Knutson : Wow.  
Jeff Horwitz : Et vous savez quoi ? Les données ont confirmé que si vous vouliez régler les problèmes sur Facebook, les problèmes d'intégrité, c'était le moyen de le faire. C'était juste des gains énormes dès le départ et cette proposition des gens qui étaient conscients de l'entendre du côté du fil d'actualité était du genre... bah, ça ne va pas arriver. Facebook n'allait pas faire ça. Il était évident que cela allait nuire gravement au business d'une manière que Facebook n'allait tout simplement pas tolérer. C'était donc mort à l'arrivée.  
https://www.wsj.com/podcasts/the-journal/the-facebook-files-part-4-the-outrage-algorithm/e619fbb7-43b0-485b-877f-18a98ffa773f 

Et ce fut loin d'être la seule proposition de la Civic Team.

22:37 Ryan Knutson : Jeff a trouvé que la Civic Team a proposé d'autres solutions comme la réduction du nombre d'invitation de groupe qu'une personne peut envoyer en une journée. La limite actuelle est de 2 250. Ou encore plafonner le nombre de commentaires que quelqu'un peut laisser, la limite actuelle est de 300 par heure.  
Jeff Horwitz : Certaines de ces choses étaient en fait assez mineures à la marge. Cela n'allait pas affecter les revenus annuels de Facebook de manière significative, même si cela allait bloquer la désinformation dans certains de ces cas. Et je pense qu'ils croyaient vraiment que tout ce qui entraîne une diminution de l'utilisation de Facebook est comme un préjudice de facto pour le produit.  
Ryan Knutson : Facebook a décidé de ne pas adopter ces changements non plus. Encore une fois, c'est parce que l'entreprise a déterminé que cela nuirait à l'engagement des utilisateurs.  
Jeff Horwitz : L'expérience du chercheur en intégrité sur ce genre de choses était que vous arriviez avec quelque chose qui semble réduire le contenu nuisible ou le mauvais comportement dans une certaine zone de la plate-forme, puis ils ont exécuté les chiffres et ont constaté que ce n'était pas bon pour les mesures de l'utilisateur, les mesures de croissance standard et alors à ce moment-là, il serait mis de côté. [...]  
Ryan Knutson : Donc le changement d'algorithme de 2018 reste en grande partie intact. 

## Zuckerberg en prison ?

Alors, personnellement, je n'ai pas d'animosité particulière envers Marc Zuckerberg. Je pense que c'est un brillant entrepreneur et technologiste, et qu'il est motivé par le prestige, la gloire et l'argent, comme beaucoup trop d'autres gens que je connais. Mais contrairement à d'autres, il a eu le bonheur, ou le malheur, d'avoir créé une plateforme avec un succès unique, qui a fait de lui, qu'il le veuille ou non, l'un des hommes les plus puissants du monde. Mais du coup, il s'est retrouvé face à de nombreux collaborateurs, investisseurs, politiciens, activistes, PDG et dictateurs, qui l'ont poussé à prendre toutes sortes de décisions qui allaient dans leurs sens. Et Zuckerberg a sans doute souvent eu l'impression de n'être qu'un pion au milieu d'un immense échiquier, où il a été contraint de privilégier les profits de Facebook à la sécurité des milliards d'utilisateurs de la plateforme. En particulier, il y a probablement des milliards d'humains qui, s'ils étaient à sa place, auraient agi plus ou moins comme lui.  
https://www.youtube.com/watch?v=rStL7niR7gs 

Il n'en reste pas moins que de nombreuses décisions qu'il a prises semblent illégales. Dès lors, il faut appliquer la loi, qui semble le conduire tout droit en prison. Et ça ne serait pas inutile. Loin de là. Faire appliquer la loi, c'est surtout s'assurer qu'elle s'appliquera encore à l'avenir, pour envoyer le signal à tous les dirigeants de toutes les entreprises du monde devront répondre des conséquences de leurs actes. J'espère en particulier que les dirigeants de Google trembleront à leur tour, le jour où ils découvriront la sentence de Zuckerberg — si sentence il y a. Voilà qui me semble être une condition nécessaire pour retrouver le contrôle sur les entreprises de l'information, et les conséquences monumentales de leurs choix de technologie.  
https://www.youtube.com/watch?v=QMIfjUerDzY

Mais emprisonner Zuckerberg et ses collègues va bien au-delà de cela encore. Il s'agit peut-être et surtout de sensibiliser toute l'industrie de l'information et le grand public aux enjeux monumentaux derrière le design des plateformes d'information, et derrière les algorithmes qui sont conçus pour prioriser telle ou telle information. Cette sensibilisation pourrait ainsi s'accompagner d'un investissement massif, aujourd'hui très déficient, dans l'audit, la sécurité et l'éthique des algorithmes déployés à grande échelle ; histoire que les entreprises ne puissent plus profiter d'une décennie de législation floue avant de retirer des algorithmes en fait très dangereux. Ceci pourrait aussi conduire à l'avènement de nouvelles régulations, bien plus à même d'encadrer adéquatement le trafic de haine et les effets des junknews sur la santé mentale de milliards d'humains à travers le monde - un peu comme on s'est mis à réguler le tabac et l'alcool.  
https://www.letemps.ch/economie/facebook-va-mettre-fin-reconnaissance-faciale-plateforme 

Mais surtout, emprisonner Zuckerberg, surtout si l'on passe via des instances comme le tribunal  pénal international, c'est enfin reconnaître que les plateformes de l'information et les algorithmes de recommandation sont devenus des terrains de guerre, et qu'il est complètement irresponsable de laisser des multinationales décider unilatéralement de la conception de ce champ de bataille, qui plus est de manière secrète, y compris pour la quasi-totalité des employés de ces entreprises. Plus que jamais, il est urgent de réfléchir à la gouvernance collaborative et internationale des réseaux sociaux - ce qui est d'ailleurs le problème que Tournesol souhaite résoudre sur le long terme.  
https://tournesol.app/ 

Enfin, à travers le cas de Zuckerberg, on pourrait reconnaître que le mythe du génie de l'innovation qui travaille sur des technologies de dernier cri, ou sur l'amélioration pure et dure des performances, ça n'a rien de “cool”. L'intelligence artificielle, la réalité virtuelle et l'ordinateur quantique posent de sérieux risques pour l'humanité, et peuvent très largement conduire à un chaos social extrêmement dangereux pour chacun d'entre nous. Dès lors, il est complètement irresponsable, et même moralement criminel, d'investir sa carrière ou 10 milliards de dollars par an dans le développement de ces technologies, et ne laisser que des miettes sur son temps libre ou quelques millions de dollars seulement pour les considérations de sécurité et d'éthique. Et je vise là ouvertement tout autant les chercheurs académiques, les fondations scientifiques, les gouvernements, les communicateurs scientifiques et les start-ups de la tech, que les géants de la Silicon Valley. Nous avons désespérément besoin d'éthique et de sécurité. Pas de nouvelles vulnérabilités technologiques.  
https://www.google.com/amp/s/www.businessinsider.com/facebook-metaverse-frances-haugen-whistleblower-shocked-by-investment-2021-10%3famp  
https://www.google.com/amp/s/www.independent.co.uk/life-style/gadgets-and-tech/facebook-meta-metaverse-harassment-moderation-b1957850.html

Bref. Vous l'aurez compris. J'aimerais apporter mon soutien public à tous ceux qui ont effectué et qui effectueront le travail très long et laborieux de monter un dossier pénal contre Facebook et ses dirigeants, et tous ceux qui soutiendront un tel travail. Un méga merci en particulier à Frances Haugen, aux lanceurs d’alerte qui l’ont précédé comme Sophie Zhang et tant d’autres, et à tous ceux qui vont la suivre, dont l'héroïsme, la rigueur et la pédagogie ont été remarquables, et pourraient s'avérer salvateurs pour toute la planète. Le futur de l'information me semble en jeu ; et par voie de conséquence, le futur de toute l'humanité aussi.

