# The real danger with #ChatGPT

3:03 "The technology is developing way too quickly for us to rely on legal instruments to fix this problem"
https://www.youtube.com/watch?v=Pm1Wgd9bbOk

You certainly know this.
Since a few months, a chatbot called #ChatGPT has been made available to the general public,
and its spectacularity has led all the major media to talk about it,
some even talking about a revolution or a disruptive technology,
often with a lot of enthusiasm.
This being said, many critical voices have been raised,
as you can see by reading the Wikipedia page of #ChatGPT.
https://fr.wikipedia.org/w/index.php?title=ChatGPT

And if you do this exercise, you will see that among the critics quoted
there is a certain Lê Nguyên Hoang...

However, the biggest danger of ChatGPT in my eyes
is not listed in this Wikipedia page,
and it is almost never mentioned in the media,
except those who invited me to talk about this danger.

And to really clarify this danger, I'll rely on a number:
$6 trillion.
6,000 billion dollars.
6,000 billion dollars is, according to the French Senate
a reasonable estimate of the annual revenue from cybercrime.
https://www.senat.fr/presse/cp20211125d.html

That's twice the GDP of France,
and 4 times more than the revenues of Google ($258B), Amazon ($470B),
Facebook ($118B), Apple ($394B) and Microsoft ($198B) combined!

Now we can estimate that ChatGPT cost a few tens of millions,
maybe a few hundred million, to be designed,
especially because of the electricity bill,
the salaries of the engineers and
the ethically questionable collection of data to "educate" ChatGPT,
as I discussed at Mr. Phi.

But mostly, given what OpenAI shared,
ChatGPT does not appear to rely on innovative techniques.
In fact, there is a whole jungle of similar language algorithms,
some of which, like Google's LaMDA and PALM, surely have comparable prowess,
and others are open source like Dumb Bloom from Hugging Face.
https://huggingface.co/bigscience/bloom

Worse still, the over-mediatization of ChatGPT's prowess has already amplified
the already massive investments in the development of these technologies,
which will certainly lead to even more spectacular algorithms than ChatGPT
in the coming years, if not months.
https://www.heidi.news/cyber/derriere-chatgpt-les-investissements-se-multiplient-pour-les-ia-generatives

However, the current legal, financial and human resources
to audit and regulate these technologies are extremely deficient,
and the industrial lobbies are using the success of ChatGPT and the race for performance
to denigrate the calls for regulation.
https://twitter.com/le_science4all/status/1615067462373965840

What's more, what ChatGPT has shown the entire cybercrime industry
is how these language algorithms could boost their business,
by automating the writing of malware,
phishing attacks, online scams,
misinformation, cyber-bullying and hate speech,
among others.

Last but not least, ChatGPT is a dangerous #CoolWashing of artificial intelligences,
and helps to distract us from its most dangerous applications,
whether it's the smart keyboards that read everything you type on your phones,
targeted advertising that exploits this and other data
to optimize private companies' appeals to overconsumption,
or algorithmic content recommendation,
which, far more than Google Search or ChatGPT, has become the primary portal
through which humans access the web.

17:06 "help me and the cybersecurity community make life much more difficult for the cybercriminals"
https://www.youtube.com/watch?v=fSErHToV8IU


## The language model landscape

To understand how wrong it would be to think of ChatGPT
to think of ChatGPT in isolation,
it is important to take a step back from the development of such algorithms.
In fact, since 2017 and a paper by Google researchers published at NeurIPS, which is one of the largest conferences on Machine learning,
language algorithms have been based on the same architecture called the Transformer.
Since then, they have continued to progress, at a spectacular pace,
so much so that ChatGPT's performance didn't really surprise me personally.
https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html

Indeed, every year, by training an algorithm 10 times bigger than the previous one
and feeding it 10 times more data,
the research teams of private groups have consistently shown that the models
that the models created in this way gained drastically in spectacularity.
In short, I personally don't feel that ChatGPT is a breakthrough,
and even less that it is the last word on the subject.
We are in a phase of exponential growth in the complexity of these algorithms,
made possible by the optimization of machines for specific calculations
of the so-called Transformer architecture.

In particular, the limiting factor today is no longer
a revolutionary algorithmic innovation,
even if it is still necessary to combine many little tricks;
the limiting factor is above all the engineering of the computing machines,
the collection of massive data and, above all, the electricity bill to make the calculations.
It is estimated that the best language models cost millions of dollars
in electricity.
https://lambdalabs.com/blog/demystifying-gpt-3#1
https://venturebeat.com/ai/openai-launches-an-api-to-commercialize-its-research/

In any case, the Bloom model designed by Hugging Face for the "open" project BigScience
project reports spending $2-5 million.
https://huggingface.co/bigscience/bloom

For ChatGPT, they may have doubled or tripled their spending,
in which case we might be talking tens or even hundreds of millions of dollars.
But probably not more, given that OpenAI only received $1 billion from Microsoft.
https://openai.com/blog/microsoft/

Anyway. My main point is that the design of language algorithms
has become mostly about money today.
In any case, if it's a matter of expertise, Google seems to be better placed than OpenAI,
https://twitter.com/le_science4all/status/1614648350057414656
since they are much more behind the main advances in the field,
both with the architecture of the Transformers,
but also with dedicated computing machines like the Tensor Processing Unit, or TPU,
or with more recent innovations like Switch Transformers or Pathways.
https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf
https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/

In fact, Google has developed spectacular chatbots before ChatGPT,
namely LaMDA and PaLM,
but they preferred not to make them available to the general public,
probably to avoid any backlash.
In fact, Google has more to lose in terms of brand image of Microsoft and OpenAI,
and that seems to me to be the main reason why they didn't bet on
on a euphoria like the one OpenAI provoked.

However, with the massive new investments in these technologies,
and with the advent of open source solutions like BLOOM,
it is to be expected that, in the coming years, we will be able to
access to very powerful language models at a reasonable cost
will become easy for malicious actors,
who can then refine these models as they see fit.

So it doesn't matter whether ChatGPT has security measures or not.
By creating euphoria, they have essentially guaranteed
the emergence of cheap, extremely powerful language models,
and easily reprogrammed by cybercrime,
so that they can continue to steal trillions of dollars,
even if it means putting hospitals, cars or banking systems at risk.
And ChatGPT-like algorithms could be particularly devastating,
especially since cybercriminals rely on language as their primary tool
to trick their victims.

16:47 "Cybercriminals [...] play with emotions [...]
More than 90% of cyberattacks are caused by human errors"
https://tournesol.app/entities/yt:4EyWrC41Oc4

10:42 "The cyberhacker is taking advantage [...] that get screwed every year."
https://tournesol.app/entities/yt:tfwASYKlypI


## What is cybercrime?

Cybercrime, precisely, let's talk about it.
And to begin with, what do we mean by "cybercrime"?
Basically, we are talking about using digital tools to commit offences,
offenses, even crimes.
And there is unfortunately a great diversity of such practices.
I propose you a small classification,
which is far from being exhaustive (and which is based on Wikipedia).

To begin with, there are the scams.
Typically, there is the famous Nigerian prince,
who asks for financial help that will be rewarded later.
But this is the scam of the year 2000.
Since then, similar but much more sophisticated scams have been developed,
such as love scams which consist in seducing singles online,
often for months,
before asking for financial help.
https://tournesol.app/entities/yt:NLwqzgniqpA

A similar attack, called the fake phone support
involves tricking the victim into thinking that their computer has been compromised,
and that they need to call a number to fix it.
Except that it is at the moment of the call that the scam really takes place,
for example because the attacker then asks the user to log in
to their online bank account by entering the password
which the attacker can then write down.
And if you want to know more,
I highly recommend this excellent investigation by Micode.
https://tournesol.app/entities/yt:gbYdQOde6EU

Overall, getting your access codes to websites is a huge cybercrime activity.
And one of the most effective ways to do this is through phishing attacks.
This consists in making you visit a page that looks like
to the registration page of a well-known site, such as GMail or Facebook.
Except that the password you enter on this page is not sent to Google or Facebook,
but is sent directly to the attacker's machines.
https://tournesol.app/entities/yt:Vz_P6HDaZlg

And then, when it is your personal account, it is already very dangerous,
especially because the attacker can then pretend to be you
to ask for financial help from your relatives.
But when it comes to your business account, this can put your business at risk,
especially if you work for a small company.
https://www.crowdstrike.fr/ressources/livres-blancs/les-realites-du-ransomware-pour-les-petites-et-moyennes-entreprises/

In particular, one of the attacks that is becoming more and more normalized on businesses
is to exploit such access to the company's information system to cripple it,
for example by encrypting all the data useful to the company's operations,
such as customer information needed to send them invoices.
The attackers will then typically demand a ransom to restore the information system.
This is known as a ransomware attack.
And as John Oliver talks about here, thousands of hospitals have already been hacked in this way,
and have sometimes had to turn away critically ill patients.
https://tournesol.app/entities/yt:WqD-ATqw3js

Terrifyingly, ransomware attacks are becoming more democratic,
so much so that a "ransomware-as-a-service" market has emerged,
where anyone can pay to launch their own attack.
https://tournesol.app/entities/yt:WqD-ATqw3js

Except that in a context of war between superpowers
like Russia, which sought to paralyze the Ukrainian electrical system,
the interests in paralyzing critical industries of enemies go far beyond simple financial profit.
Therefore, we must fear a catastrophic cyberwar,
especially if the tools needed to carry out a large-scale attack
become affordable to a very large number of groups,
we should potentially fear even more the risks of cyberterrorism.
https://api.tournesol.app/entities/yt:yfr0BVCMAZA

But much of cybercrime is much more vicious and discreet.
In particular, so-called advertising, identity and attribution frauds involve
typically design, steal or buy up a large number of fake accounts,
and then produce a massive amount of misinformation,
to inflate the popularity, prices or rates of certain products,
to amplify the click rate, and thus the recommendation rate, of certain contents,
or to massively report certain critical contents,
for the benefit of governments, political parties or private companies.
https://tournesol.app/entities/yt:V-1RhQ1uuQ4

These accounts, nowadays often controlled by troll farms,
also called click factories,
are also used to produce shitstorms and massively harass certain targets.
This same harassment can also abuse deep fakes to damage the image of these targets,
even to their mental well-being,
notably by creating pornographic deep fakes.
https://www.huffingtonpost.co.uk/entry/deepfake-porn_uk_5bf2c126e4b0f32bd58ba316

Love scam, fake phone support, phishing, ransomware, cyberwarfare,
cyberterrorism, click farms, waves of harassment,
even disinformation and hate speech.
This list of cybercrime is far from exhaustive, but it is already terrifying.
And clearly, language algorithms like ChatGPT will make it much easier for those who want to take advantage
for those who want to profit from these illegal attacks,
and make it much more difficult for cybersecurity professionals to do so.
https://www.theregister.com/2023/01/18/russia_openai_chatgpt_workarounds/

Normalizing the #CoolWashing of ChatGPT without insisting
the civilizational dangers of the democratization of language algorithms,
and the urgent need for monumental resources to protect societies from cybercrime,
is to make the whole population even more vulnerable to major disruptions
of the many critical systems on which our societies depend so much,
whether it's hospitals, the electrical grid, the banking system,
and the fabric of small and medium-sized businesses.

4:11 "48% of us work for small businesses [...] this is bad news, guys."
https://tournesol.app/entities/yt:c_2Ja-OTmGc


## The cybercrime market

OK. Cybercrime can be very dangerous.
But how big is its market?
Are we really to believe that advances in AI will be exploited more often
by malicious actors than for noble purposes
such as medical applications?

Not only is the size of this market a huge #MuteNews,
but what is probably just as poorly measured,
is its spectacular growth.
Estimated at a total cost of $3 trillion in 2015 by Cybersecurity Ventures,
it appears to have reached $6 trillion by 2021,
and could reach $10 trillion by 2025.
https://www.youtube.com/watch?v=P6x4GhjDVHY
https://cybersecurityventures.com/annual-cybercrime-report-2019/

As a comparison, from my research which I admit is a bit sketchy,
and therefore to which I am far from assigning much credence,
the illegal arms market would represent "only" 60 billion dollars per year.
https://www.rankred.com/10-biggest-illegal-businesses-around-the-world/
The illegal drug market, on the other hand
is estimated at 500 billion dollars per year according to some sources,
https://www.talkingdrugs.org/report-global-illegal-drug-trade-valued-at-around-half-a-trillion-dollars
while the counterfeiting market is estimated to be worth more than a thousand billion dollars.
https://www.rankred.com/10-biggest-illegal-businesses-around-the-world/

All this to say that shady individuals willing to break the law to get rich
actually have a vested interest in investing more in cybercrime
than in those other illegal markets that have become all too common.
And obviously, a lot of them are doing it.

Yes, because, to be clear, given the scale of the stakes and the potential profits
cybercrime is far from being a matter of isolated kids
on their machines in their parents' garage.
It is now a real business, with suppliers and customers.
For example, in 2012, and according to Wikipedia, the predominant threat on the web
was not a virus, but a kit that customers could buy
to attack their victims themselves.
https://en.wikipedia.org/wiki/Blackhole_exploit_kit

In any case, as early as 2015, Gini Rometty, then CEO of IBM, stated that, and I quote,
"cybercrime is the biggest threat to every profession,
every industry and every business in the world."
https://www.csoonline.com/article/3210912/is-cybercrime-the-greatest-threat-to-every-company-in-the-world.html

In 2017, a cybersecurity expert polled his audience on Twitter
to see if they agreed with this observation.
The majority agreed with this message from Rometty.
https://twitter.com/CybersecuritySF/status/888805918799998976

Imagine if cars, the power grid, hospitals or the banking system,
but also, more simply, the reservation systems of the SNCF,
or even government databases,
were suddenly paralyzed by malicious actors,
who would demand huge ransoms to make these systems work again,
or if billions of deepfakes asked your fellow citizens to perform some obscure action on the web,
pretending to be their relatives, their colleagues or even their bosses.

20:45 "Please don't believe [...] you need to verify."
https://tournesol.app/entities/yt:BQ_JrFgUTKI

## Conclusion

Faced with this realistic discourse of the landscape of truly deployed algorithms,
AI enthusiasts often rush to minimize the responsibility of algorithms.
Many will thus compare these algorithms to knives.
Yes, algorithms can be used to kill.
But should they be banned?
In the case of a knife murder, isn't the fault of the human handling the knife?

What I have tried to convince you of here
is that the comparison between ChatGPT and a knife is clumsy,
especially from a consequentialist point of view.
In particular, the likely consequences of developing a knife,
especially in a society where knives are already widely available
and are much more likely to be used for cooking and eating,
do not seem at all comparable to the likely consequences of the development of ChatGPT,
which normalizes and democratizes tools that are much more likely to be used by
by cybercrime than for purposes useful to the well-being and security of the general public.

In other words, the problem of the precipitous development and deployment of AI seems to me to be contextual.
In the world we live in today
with its ecological and economic constraints,
and especially with the uncontrolled proliferation of cybercrime
and geopolitical instability on a global scale,
accelerating the development of super-powerful algorithms to do everything
seems to me to be much more dangerous than
slowing down this development and investing massively in regulation,
all the more so as the beneficial applications for humanity seem to me very limited and unconvincing.
In other words, the risk-benefit balance seems to me to be very much in favor of the risks.

Unfortunately, this risk-benefit balance is often very far from the minds of many.
The #CoolWashing of AI,
but also the insistence on talking about secondary issues to me,
like the role of "humans", the non-negligible but not insane carbon footprint of AI,
or the fact that these technologies are not often designed in Europe,
all this seems to me to have led to a serious lack of distrust from the general public,
but also from researchers, journalists and politicians.
In the current context where cybercrime predominates, these algorithms are extremely dangerous,
and we should urgently invest much more in their regulation and auditing.
And that requires not only laws,
but it requires even more important investments
in cybersecurity agencies such as the Agence Nationale de la Sécurité des Systèmes d'Information,
and in public research on cybersecurity,
which, I believe, should prioritize the recruitment of security-oriented researchers
than experts in learning system optimization without attention to security.

But above all, the biggest danger of the excessive attention given to OpenAI,
seems to me to be the inattention it causes to the most dangerous algorithms,
which in my opinion are the advertising targeting and content recommendation algorithms.
These algorithms are already widely deployed,
have caused social chaos in the United States, Brazil and the United Kingdom,
and genocides in Myanmar, Ethiopia and maybe soon in India.
https://www.aljazeera.com/news/2022/1/16/expert-warns-of-possible-genocide-against-muslims-in-india

In particular, in a very real way,
while billions in philanthropic funds have been poured into companies like OpenAI,
alternatives dedicated to ethics, security and collaborative governance
of the most dangerous algorithms on the web like Tournesol
are severely lacking in donations and visibility, both in research and in the media.
In the case of Tournesol, after now almost 3 years,
we only have enough money to pay a developer's salary;
and even with the best ideas,
it's hard to compete with OpenAI, Google or TikTok with only 1 employee,
especially since the development of the secure algorithms we need
is in fact more sophisticated, at least mathematically,
than that of #ChatGPT, LaMDA or the "magic button" of TikTok.
https://tournesol.app/entities/yt:RMJz3nVE-78

Fortunately, at the research level anyway,
ethical, security and governance issues are becoming increasingly important.
But you can help us accelerate this trend,
by using our Tournesol extension for Chrome and Firefox,
by contributing to identify more top-quality videos,
and most importantly, by promoting the platform to you and to journalists.
It is by encouraging these kinds of initiatives that you can push technologies
to be designed with more consideration for the safety and well-being of the greatest number.

03:45 "I think that the next big technology should be the creation of a safer Internet".
https://www.youtube.com/watch?v=Pm1Wgd9bbOk
