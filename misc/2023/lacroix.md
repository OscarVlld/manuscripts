# L'urgence à adopter la présomption de non-conformité en IA
5000 signes.

Le 3 mai 2023, l'entreprise technologique Samsung a [interdit](https://www.journaldugeek.com/2023/05/03/chatgpt-pourquoi-samsung-interdit-a-ses-employes-dutiliser-lia/) à ses employés l'utilisation de la nouvelle génération d'intelligences artificielles (IA) conversationnelles, comme ChatGPT, Google Bard et Bing Chat.
Cette décision suggère vivement que ces IA peuvent être davantage des vulnérabilités pour les entreprises que des atouts.
En l'occurence, plusieurs employés de Samsung avaient demandé à ChatGPT d'analyser du code source et de résumer des notes de réunion.
Or ce faisant, ils ont communiqué à OpenAI ces secrets industriels.
Pire encore, les prochaines versions de ChatGPT apprendront ces données, et [risquent de les relayer à leurs millions d'utilisateurs](https://arxiv.org/abs/2304.05197).
Selon Samsung, OpenAI est une menace sérieuse pour la protection des informations sensibles.

Pourtant, pendant ce temps,
la commission nationale de l'informatique et des libertés (CNIL),
chargée d'appliquer le règlement générale pour la protection des données (RGPD),
n'a toujours pas réagi, 
malgré la réception d'au moins [5 plaintes](https://www.lesechos.fr/tech-medias/hightech/chatgpt-la-cnil-enquete-sur-cinq-plaintes-1934550).

Cette étrange situation illustre un problème plus général.
Alors que les nouvelles technologies de l'information bouleversent nos sociétés à une vitesse spectaculaire,
leur régulation est arriérée.
En particulier, la *présomption de non-conformité*,
déjà abondamment appliquée dans les industries matûres
comme l'aviation, la pharmaceutique et l'agroalimentaire,
doit urgemment être adoptée au numérique aussi.


## Les dangers de la présomption de conformité

La plus grande déficience de la régulation actuelle, 
c'est sans doute le fait que les régulateurs du numérique n'interviennent essentiellement jamais 
en amont du déploiement des technologiques.
C'est même probablement après l'adoption massive de ChatGPT, TikTok ou du Bitcoin, 
que la question de la conformité de ces systèmes d'information
avec les lois *déjà existantes* s'est posée.
Pire encore, la charge de la preuve de non-conformité retombe trop souvent sur la partie civile.
Cette situation est inacceptable, pour de nombreuses raisons.

Premièrement, la société civile a rarement accès à des données incriminantes.
En fait, c'est souvent grâce au courage de lanceurs d'alerte ou à des fuites de données,
que ces données incriminantes sont apprises par les régulateurs,
à l'instar des [Facebook Files](https://www.la-croix.com/Economie/Frances-Haugen-lanceuse-dalerte-issue-Facebook-tournee-europeenne-2021-11-07-1201184083) révélés par Frances Haugen, 
qui montrent le rôle dévastateur des algorithmes de Facebook pour les démocraties.

Deuxièmement, cette situation encourage de plus l'opacité des entreprises.
Beaucoup de mystères entourent ainsi le dernier algorithme GPT-4 d'OpenAI.
De façon plus préoccupante encore, 
Google, Meta et Microsoft ont successivement démantelé leurs équipes d'éthique,
et [restreint l'accès](https://www.wsj.com/articles/facebook-limits-employee-access-to-some-internal-discussion-groups-11634171786) à des discussions internes à ses employés,
réduisant ainsi leur transparence en interne.

Troisièmement, l'inaction des régulateurs encourage le développement précipité des algorithmes
par toutes sortes d'acteurs, non seulement Google, Microsoft et Meta,
mais aussi Baidu, Hugging Face ou encore le Technology Innovation Institute d'Abu Dhabi.
Cette précipitation est en particulier associée à une réduction alarmante des standards de sécurité,
[bien documentée dans le cas de Google](https://www.bloomberg.com/news/features/2023-04-19/google-bard-ai-chatbot-raises-ethical-concerns-from-employees#xj4y7vzkg).

Quatrièment, les technologies du numérique restent massivement utilisées
tant que les régulateurs n'agissent pas.
Si elles présentent des risques, chaque jour d'inaction légale met des entreprises ou des vies en danger,
à l'instar des conséquences de TikTok sur la santé mentale de ses utilisateurs.

Enfin, l'inapplication de lois déjà existantes, faute par exemple d'enquêtes dignes de ce nom, décrédibilise toutes les lois.
À l'heure où l'Europe s'arme de nouveaux textes légaux très exigeants,
comme l'AI Act ou le Cyber Resilience Act,
la question des moyens pour les faire appliquer se pose sérieusement.

## L'IA ne doit pas faire exception

Il existe toutefois une parade très simple à tous ces problèmes,
déjà abondamment appliqués à travers les industries matûres, 
à savoir la *présomption de non-conformité*.
Typiquement, par défaut, un nouvel avion d'Airbus n'a pas le droit de vol.
Et c'est aux concepteurs de convaincre des régulateurs puissants, bien financés et indépendants
de la conformité de l'avion avec les réglementations notamment liées à la sécurité de l'appareil.

En finance, un principe similaire de *présomption de blanchiment* 
a d'ailleurs été récemment introduit pour les montages financiers douteux,
rejetant ainsi la *charge de la preuve* à ceux qui possèdent les données les plus pertinentes
pour déterminer la légalité de ces montages.
Cette disposition du code pénale a été [qualifiée](https://www.lemonde.fr/les-decodeurs/article/2023/04/29/la-presomption-de-blanchiment-arme-redoutable-contre-les-montages-financiers-occultes_6171473_4355770.html) de « game changer » par des enquêteurs et des magistrats « dithyrambiques ».

Alors que les incidents liés à TikTok et ChatGPT, mais aussi à Facebook, YouTube et Twitter, 
ne cessent de se multiplier,
et que les données les plus pertinentes pour déterminer la légalité de ces systèmes restent le monopole des entreprises qui développent ces produits,
il paraît urgent que les régulateurs du numérique adoptent eux aussi
la présomption de non-conformité.

