# Faut-il interdire ChatGPT ?

Il y a quelques semaines, des employés de Samsung ont demandé à ChatGPT d'analyser du code et de résumer des notes de réunion,
communiquant ainsi des secrets industriels à OpenAI, l'entreprise qui développe ChatGPT.
Dès lors, les prochaines versions de ChatGPT auront sans doute appris ces données, et [risquent de les relayer à leurs millions d'utilisateurs](https://arxiv.org/abs/2304.05197).
Suite à ces événement, Samsung a [interdit](https://www.journaldugeek.com/2023/05/03/chatgpt-pourquoi-samsung-interdit-a-ses-employes-dutiliser-lia/) à ses employés l'utilisation des intelligences artificielles (IA) conversationnelles, comme ChatGPT, Google Bard et Bing Chat.
Selon Samsung, ces IA sont une menace sérieuse pour la protection des informations sensibles.

Pourtant, pendant ce temps,
la commission nationale de l'informatique et des libertés (CNIL),
chargée notamment d'appliquer le règlement général pour la protection des données (RGPD),
tarde à réagir, 
malgré la réception de [5 plaintes](https://www.lesechos.fr/tech-medias/hightech/chatgpt-la-cnil-enquete-sur-cinq-plaintes-1934550).

Cette étrange situation illustre un problème plus général.
Alors que les nouvelles technologies bouleversent nos sociétés,
leur régulation est arriérée.
En particulier, la *présomption de non-conformité*,
déjà abondamment appliquée dans les industries matûres
comme l'aviation, la pharmaceutique et l'agroalimentaire,
est cruellement déficiente à la régulation numérique.


## La régulation traîne

En effet, les régulateurs du digital n'interviennent presque jamais 
en amont du déploiement des technologies.
C'est même probablement après l'adoption massive de ChatGPT, TikTok ou du Bitcoin, 
que la question de la conformité avec les lois *déjà existantes* s'est posée.
Pire encore, la charge de la preuve de non-conformité retombe trop souvent sur la partie civile.
Cette situation est inacceptable, pour de nombreuses raisons.

Premièrement, la société civile a rarement accès à des données incriminantes.
En fait, c'est souvent grâce à des fuites de données ou au courage de lanceurs d'alerte,
que ces données incriminantes sont apprises par les régulateurs,
à l'instar des [Facebook Files](https://www.la-croix.com/Economie/Frances-Haugen-lanceuse-dalerte-issue-Facebook-tournee-europeenne-2021-11-07-1201184083) révélés par Frances Haugen, 
qui montrent le rôle dévastateur des algorithmes de Facebook sur les démocraties.

Deuxièmement, cette situation encourage l'opacité des entreprises.
Beaucoup de mystères entourent ainsi le dernier algorithme GPT-4 d'OpenAI.
De façon plus préoccupante encore, 
[Google, Meta et Microsoft ont successivement démantelé leurs équipes d'éthique](https://techcrunch.com/2023/03/13/microsoft-lays-off-an-ethical-ai-team-as-it-doubles-down-on-openai/),
et [restreint l'accès](https://www.wsj.com/articles/facebook-limits-employee-access-to-some-internal-discussion-groups-11634171786) à des discussions internes à ses employés,
réduisant ainsi leur transparence en interne.

Troisièmement, l'inaction des régulateurs encourage des développements précipités, 
non seulement Google, Microsoft et Meta,
mais aussi Baidu, Hugging Face ou encore le Technology Innovation Institute d'Abu Dhabi.
Cette précipitation s'accompagne d'une réduction alarmante des standards de sécurité,
[bien documentée dans le cas de Google](https://www.bloomberg.com/news/features/2023-04-19/google-bard-ai-chatbot-raises-ethical-concerns-from-employees#xj4y7vzkg).

Quatrièment, les technologies du numérique restent massivement utilisées
tant que les régulateurs n'agissent pas.
Si elles présentent des risques, chaque jour d'inaction légale met davantage d'entreprises et de vies en danger,
notamment dans le cas de [la santé mentale des utilisateurs de TikTok](https://jonathanhaidt.substack.com/p/international-mental-illness-part-one).

Enfin, l'inapplication de lois déjà existantes, faute par exemple d'enquêtes dignes de ce nom, décrédibilise toutes les lois.
À l'heure où l'Europe s'arme de nouveaux textes légaux très exigeants,
comme [l'AI Act](https://artificialintelligenceact.eu/) ou le [Cyber Resilience Act](https://digital-strategy.ec.europa.eu/en/library/cyber-resilience-act),
la question des moyens pour les faire appliquer se pose sérieusement.

## Le numérique ne doit pas faire exception

Il existe toutefois une parade simple et efficace à ces problèmes,
déjà abondamment appliquée dans les industries matûres, 
à savoir la *présomption de non-conformité*.
Typiquement, par défaut, un nouvel Airbus n'a pas le droit de vol.
Et c'est à l'entreprise de démontrer aux régulateurs la conformité de leur nouveau produit 
avec les réglementations déjà existantes.

En finance, un principe similaire de *présomption de blanchiment* 
a d'ailleurs été récemment introduit pour les montages financiers douteux,
rejetant ainsi la *charge de la preuve* à ceux qui possèdent les données les plus pertinentes
pour déterminer la légalité des opérations financières.
Cette disposition a été [qualifiée](https://www.lemonde.fr/les-decodeurs/article/2023/04/29/la-presomption-de-blanchiment-arme-redoutable-contre-les-montages-financiers-occultes_6171473_4355770.html) de « game changer » par des enquêteurs et des magistrats « dithyrambiques ».

Alors que les incidents liés à TikTok et ChatGPT, mais aussi à Facebook, YouTube et Twitter, 
ne cessent de se multiplier,
et que les données les plus pertinentes pour déterminer la légalité de ces systèmes demeurent le monopole des entreprises qui développent ces produits,
il paraît urgent que les régulateurs du numérique adoptent eux aussi
la présomption de non-conformité.

